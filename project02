{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":2346296,"sourceType":"datasetVersion","datasetId":1416444}],"dockerImageVersionId":30626,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"#### MOVIE RATING PREDICTION WITH PYTHON\n- Build a model that predicts the rating of a movie based on features like genre, director, and actors. You can use regression techniques to tackle this problem.\n- The goal is to analyze historical movie data and develop a model that accurately estimates the rating given to a movie by users or critics.\n- Movie Rating Prediction project enables you to explore data analysis, preprocessing, feature engineering, and machine learning modeling techniques. It provides insights into the factors that influence movie ratings and allows you to build a model that can estimate the ratings of movies accurately.\n\n#### Dataset - https://www.kaggle.com/datasets/adrianmcmahon/imdb-india-movies","metadata":{}},{"cell_type":"markdown","source":"### Steps\n#### Preparing the environment\n1. Importing the necessary packages and modules\n2. Reading the required files in a Data Frame\n    - We can see that only rating is of float datatype and all the other columns are of objcect datatype\n    - Also we see alot missing values throughout\n#### Date Preprocessing\n3. We see that there alot of missing values in the Target column itself.\n    - Droping the rows with missing values in the target column.\n    - We cannot train a regression model with missing target values, so droping them from the training set.\n    - Considering the rows with missing values in target as the test dataset.\n3. Train and test (validation) split\n4. Checking for duplicate rows\n    - 6 duplicate rows were removed from the test data frame.\n5. Missing values treatment.\n    - We see that there are alot mire missing values in the test datframe, rather than in the other dtaframes.\n    - Use the train and validation dataframes to fill the missing values in the test dataframe.\n    - Filling the missing values in Year column\n        - We see that there are no missing values in Training and Validation datasets.\n        - Coverting the values in column from string to integer\n        - Filling the null values in test dataset with mode of train, even tough it is an int feature the column is categorical.\n    - Filling the missing values in duration Duration column.\n        - We see missing values in all datasets.\n        - Converting the duration to float.\n        - Checking for outliers, if there are outliers needs to filled with median if not with mean.\n        - We see outliers in all datasets, so filling the misssing values to median.\n    - Filling the missing values in Genre column.\n        - Spliting the genre column into 3 columns.\n        - Checking the missing values in each column of genre to decide whether to drop or impute the column.\n        - Using mode imputaion method to fill the missing values based on the mode of a particular year.\n        - Filling the missing values in the genre column of training dataset with the mode genre of training dataset.\n    - Missing values in Votes columns\n        - We see that there are no missing values for Votes columns, as the rating is provided based on the votes.\n        - There are only missing values in Votes column in test datset, as there are no ratings available.\n        - We cannot impute the Votes values in the test dataset, as that is connected to the target variable.\n        - We first need to see the significance of that column and then need to decide if it needs to be droped or not.\n        - For now we can either leave the column as it is or chnage everthing as 0, it can be decided only after building and checking each scenario.\n        - There is only one non-null value in test datset, that also seems to be a wrong value, that is changed back to nan.\n    - Deciding to fill the missing values in the Director and Actors column with 'Not Available', if there is any impact because of this then, need to check and do the necessary changes.\n#### Exploratory Data Analysis\n6. Checking the correlation of the filled dataset.\n    - After checking the correlation and other viz, we see nothing unusual.\n7. Checking the other visualisations to explore our dataset and analysing the data we notice the below trends.\n    - The movie name \"Anjaam\" has been used in 5 movies.\n    - The most number of movies were released in the year 2019.\n    - Director \"Mahesh Bhatt\" has directed the most number of movies.\n    - \"Jeetendra\" has been the lead actor in most of the movies.\n    - But we can see that \"Mithun Chakraborty\" is in the top 10 list of all actor categories, making him the most predominant actor.\n    - \"Rehka\" has been the lead actress throughout.\n    - The movie genres Drama and Action are far ahead of other genres.\n    - Checking the average rating per year, we see that the movies released between 1980 to 2010 are prety low and the trend is slowly changing as there is consistent increase in average ratingsa after 2000. \n    - This may relate to the technological developments at the end of 20th Century, and how Indian cinema adapted to these technologies over the turn of the century.\n    - Surprisingly we can see that the movies with duration around the median time all have lower ratings compared to the ones at the either end.\n    - We see very dense votes under 10,000, making it difficult to come to any conclusions based of the number of votes.\n    - Seeing the top 20 rated Directos and ACtors, we see that the highest rated doesn't have much of works under their name.\n    - Simply saying the top rated Actors or Directors have very worked on very less number of movies.\n    - Obviously the average rating of Drama genre is the highest this may be just because of the sheer number of drama movies.\n    - Music genre having the next best ratings is quite surprising as compared to Family and Romance movies that are popular in India.\n    - We see that the top rated movies overall have very low votes compared to the other mvies with thousands of votes.\n    - We also see that the years 2020 and 2021 have the most number of top rated movies 7 of top 10\n    - We also see that the all of the top rated movies are after 2017\n    - We can see that there is increase in the number of popular movies over the years and specially after the year 2000 we see a considerable increase compared to the previous years.\n    - We see that 3 idiots is by far the mosed voted best rated movie overall in Indian cinema\n#### Futher Preprocesing and model building\n8. Encoding all the categorical values before building the models.\n    - Using target encoding as there alot of unique variables.\n9. After checking the correlation and pairplot, desided to proceed with all columns\n10. Doing the dependent and independent variable split for model training.\n#### Building the models\n11. Training various models on the raw data checked the r2 scores amd mean squared error values\n    - Ensemble models like Random Forest, Gradient Boost Regressor, LightGBM, and XGBoost show good performance with relatively low MSE and high R-squared scores on the test set.\n    - Decision Tree performs well but may have a slightly higher MSE compared to ensemble methods.\n    - Linear Regression and Ridge Regression show similar performance, indicating that L2 regularization might be more suitable for the data.\n    - Lasso Regression and Elastic Net Regression seem to have slightly higher MSE and lower R-squared scores compared to other models.\n    - KNN Regressor appears to perform less well on the dataset, with a higher MSE and lower R-squared scores.\n    - Checking the above we can see that Random Forest, Gradient Boosting, LGBoost and XGBoost have all performed better\n    - They all have men squared erros less than 0.5 \n    - All four hae r2 score of more than 0.75 for both train and test\n    - Support Vector Regression techniques are computationaly very high and low performance\n12. Handling outliers and building the models to see if the accuracy can be improved\n    - We see outliers and skewness throughout the dataset\n        - Leaving the Year column as it is\n        - Using IQR method on the other columns\n    - Building the prediction models after treating the outliers.\n    - Outlier treatment has led to changes in performance metrics for some models compared to the evaluation on raw data.\n    - Ensembles like Random Forest and Gradient Boost Regressor still perform well after outlier treatment, with relatively low MSE and high R-squared scores.\n    - Support Vector Model with a Linear Kernel and KNN Regressor show a decrease in performance on the test set after outlier treatment.\n    - Linear models (Linear Regression, Lasso, Ridge, Elastic Net) also show consistent or improved performance after outlier treatment.\n    - Checking the above we can see that Random Forest, Gradient Boosting, LGBoost and XGBoost have all performed better\n    - They all have men squared erros less than 0.5 \n    - All four hae r2 score of more than 0.75 for both train and test\n    - Additionally we have some more models reaching 0.7 accuracy\n    - Support Vector Regression techniques are computationaly very high and low performance, even after outlier treatment\n13. Scaling the raw data and running the models on them\n    - Using the Standared scaler method on the raw data.\n    - Tree-based models (Random Forest, Gradient Boost Regressor, LightGBM, XGBoost) and linear models (Linear Regression, Lasso, Ridge, Elastic Net) perform well on the test set.\n    - Support Vector Models with a linear and RBF kernel show good performance, while those with polynomial and sigmoid kernels exhibit poor performance, possibly indicating overfitting or inappropriate kernel choices.\n    -  KNN Regressor shows poor performance with negative R-squared scores, suggesting that it might not be suitable for the given data.\n    - Linear Regression, Lasso Regression, and Ridge Regression show similar performance, with decent R-squared scores on the test set.\n    - Decision Tree and Random Forest perform reasonably well, with Random Forest outperforming Decision Tree in terms of R-squared scores.\n    - Gradient Boost Regressor, LightGBM, and XGBoost show good performance, with R-squared scores around 0.82 on the test set.\n    - KNN Regressor seems to perform poorly in comparison to the other models, with negative R-squared scores indicating that it might not be well-suited for the data.\n14. Scaling the outlier treated dataset to see if the performance can be improved.\n    - Using Standard Scaler on the outlier treated data\n    - Gradient Boost Regressor and Random Forest perform well on both train and test sets, indicating good generalization.\n    - Support Vector Models with linear and RBF kernels show good performance.\n    - Polynomial and sigmoid kernels in Support Vector Models exhibit poor performance, possibly indicating overfitting or inappropriate kernel choices.\n    - KNN Regressor shows good performance on the scaled and outlier-treated data.\n15. Overall comment on all the models\n    - Tree-based models (Decision Tree, Random Forest, Gradient Boost Regressor, LightGBM, XGBoost) consistently outperform linear models and KNN.\n    - Scaling generally improves the performance of linear models.\n    - Outlier treatment has a positive impact on some models but not uniformly across all.\n    - Support Vector Machines (SVM) with RBF kernel perform well on scaled data.\n    - XGBoost consistently demonstrates strong performance across different data preprocessing techniques (raw, outlier treated, scaled, scaled outlier treated). It achieves high R-squared scores on the test set, indicating good predictive capability.\n    - Similar to XGBoost, Random Forest consistently performs well across different scenarios. It provides a good balance between accuracy and interpretability. The model is robust and less prone to overfitting.\n    - The Gradient Boost Regressor consistently shows strong performance and is known for its ability to capture complex relationships in the data. It performs well across different preprocessing techniques.\n16. Enhancing the top 3 models with cross validation to confirm which data set to build the final model on\n    - Random Forest with scaled raw and scaled outlier treated data\n        - MSE - 0.318\n        - R2 Score - 0.831\n    - Gradient Boosting with scaled outlier treated data\n        - MSE - 0.330\n        - R2 Score - 0.825\n17. After cross validation we come to the below conclusions.\n    - The Random Forest models with both scaled raw data and scaled outlier-treated data have similar performance, with high accuracy on both training and test sets.\n    - Gradient Boosting also performs well, with slightly higher mean accuracy on the training set compared to Random Forest.\n    - The elapsed time for cross-validation is quite short, indicating efficient execution.\n    - But considering all the factors Random Forest model with scaled raw data seems to be a strong performer:\n        - Test Mean Accuracy: 84.59%\n        - Test Max Accuracy: 86.54%\n        - Elapsed Time: 0.0222 seconds\n#### 18. Futhur Recomendations\n   - We can use multiple linear regression techniques to find votes and rating for all the movies with missing rating and votes.\n   - We can drop the votes column to train the model, as the significance is not that high and predict the rating of all movies with missing ratings","metadata":{}},{"cell_type":"code","source":"# Importing the necessary packages and modules\n\n# !pip install dtale\n!pip install category encoders\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport category_encoders as ce\n# import dtale\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport seaborn as sns\nimport statsmodels.api as sm\nimport statsmodels.regression.linear_model as smf\nimport timeit\nimport xgboost as xgb\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom numba import jit, cuda\n# from pandas_profiling import ProfileReport\n# from pycaret.classification import *\n# from pycaret.regression import *\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso, LinearRegression, Ridge\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom statsmodels.formula.api import ols\nfrom statsmodels.regression.linear_model import OLS\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom wordcloud import WordCloud\n\nsns.set()\n%matplotlib inline","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:28.029948Z","iopub.execute_input":"2023-12-17T18:38:28.031341Z","iopub.status.idle":"2023-12-17T18:38:42.685293Z","shell.execute_reply.started":"2023-12-17T18:38:28.031284Z","shell.execute_reply":"2023-12-17T18:38:42.68374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Reading the dataset into a data frame\n# Getting error as ''utf-8' codec can't decode byte 0xe1 in position 76763: invalid continuation byte'\n# So using 'encoding = 'latin-1'\n\nmovies_df = pd.read_csv(\"/kaggle/input/imdb-india-movies/IMDb Movies India.csv\", encoding = 'latin-1')\nmovies_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.687881Z","iopub.execute_input":"2023-12-17T18:38:42.688327Z","iopub.status.idle":"2023-12-17T18:38:42.771181Z","shell.execute_reply.started":"2023-12-17T18:38:42.688288Z","shell.execute_reply":"2023-12-17T18:38:42.770131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.772375Z","iopub.execute_input":"2023-12-17T18:38:42.772709Z","iopub.status.idle":"2023-12-17T18:38:42.803661Z","shell.execute_reply.started":"2023-12-17T18:38:42.772679Z","shell.execute_reply":"2023-12-17T18:38:42.802217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"movies_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.807286Z","iopub.execute_input":"2023-12-17T18:38:42.80765Z","iopub.status.idle":"2023-12-17T18:38:42.825629Z","shell.execute_reply.started":"2023-12-17T18:38:42.807618Z","shell.execute_reply":"2023-12-17T18:38:42.824577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for duplicate rows\n\nmovies_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.8272Z","iopub.execute_input":"2023-12-17T18:38:42.827637Z","iopub.status.idle":"2023-12-17T18:38:42.866783Z","shell.execute_reply.started":"2023-12-17T18:38:42.827605Z","shell.execute_reply":"2023-12-17T18:38:42.865713Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Only one column is of float datatype, all the other columns are of object datatype.\n- We see a lot of missing values in the target column.\n- Creating a new training data frame where there are missing values in the target column\n- We see there are 6 duplicate rows in our dtaset","metadata":{}},{"cell_type":"code","source":"# Spliting the data based on target column\n# If the target column is nan or null then it will go into the test df\n\nrest_df, test_df = [x for y, x in movies_df.groupby(movies_df['Rating'].isna())]","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.868202Z","iopub.execute_input":"2023-12-17T18:38:42.868944Z","iopub.status.idle":"2023-12-17T18:38:42.883756Z","shell.execute_reply.started":"2023-12-17T18:38:42.868874Z","shell.execute_reply":"2023-12-17T18:38:42.882419Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.885821Z","iopub.execute_input":"2023-12-17T18:38:42.886645Z","iopub.status.idle":"2023-12-17T18:38:42.906741Z","shell.execute_reply.started":"2023-12-17T18:38:42.886597Z","shell.execute_reply":"2023-12-17T18:38:42.905838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rest_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.908162Z","iopub.execute_input":"2023-12-17T18:38:42.909094Z","iopub.status.idle":"2023-12-17T18:38:42.926524Z","shell.execute_reply.started":"2023-12-17T18:38:42.909057Z","shell.execute_reply":"2023-12-17T18:38:42.925196Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Train test (validation) split\n\ntrain_df, validation_df = train_test_split(rest_df, train_size = 0.75, random_state = 101)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.928197Z","iopub.execute_input":"2023-12-17T18:38:42.92891Z","iopub.status.idle":"2023-12-17T18:38:42.9561Z","shell.execute_reply.started":"2023-12-17T18:38:42.928817Z","shell.execute_reply":"2023-12-17T18:38:42.954794Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.961066Z","iopub.execute_input":"2023-12-17T18:38:42.961451Z","iopub.status.idle":"2023-12-17T18:38:42.978405Z","shell.execute_reply.started":"2023-12-17T18:38:42.961418Z","shell.execute_reply":"2023-12-17T18:38:42.976976Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for duplicate rows\n\ntrain_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:42.980193Z","iopub.execute_input":"2023-12-17T18:38:42.980557Z","iopub.status.idle":"2023-12-17T18:38:43.008187Z","shell.execute_reply.started":"2023-12-17T18:38:42.980527Z","shell.execute_reply":"2023-12-17T18:38:43.006993Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.010774Z","iopub.execute_input":"2023-12-17T18:38:43.011276Z","iopub.status.idle":"2023-12-17T18:38:43.029636Z","shell.execute_reply.started":"2023-12-17T18:38:43.011229Z","shell.execute_reply":"2023-12-17T18:38:43.028748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.030604Z","iopub.execute_input":"2023-12-17T18:38:43.030955Z","iopub.status.idle":"2023-12-17T18:38:43.058278Z","shell.execute_reply.started":"2023-12-17T18:38:43.030925Z","shell.execute_reply":"2023-12-17T18:38:43.057145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that all the duplicate values are in test dataframe, proceeding to remove the duplicates.","metadata":{}},{"cell_type":"code","source":"test_df[movies_df.duplicated(keep = False)]","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.059657Z","iopub.execute_input":"2023-12-17T18:38:43.060052Z","iopub.status.idle":"2023-12-17T18:38:43.112592Z","shell.execute_reply.started":"2023-12-17T18:38:43.060019Z","shell.execute_reply":"2023-12-17T18:38:43.111414Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Removing the duplicate values\n\ntest_df.drop_duplicates(inplace = True)\ntest_df.duplicated().sum()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.114248Z","iopub.execute_input":"2023-12-17T18:38:43.114628Z","iopub.status.idle":"2023-12-17T18:38:43.159728Z","shell.execute_reply.started":"2023-12-17T18:38:43.114595Z","shell.execute_reply":"2023-12-17T18:38:43.158718Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.161134Z","iopub.execute_input":"2023-12-17T18:38:43.16146Z","iopub.status.idle":"2023-12-17T18:38:43.183744Z","shell.execute_reply.started":"2023-12-17T18:38:43.16143Z","shell.execute_reply":"2023-12-17T18:38:43.18231Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.185864Z","iopub.execute_input":"2023-12-17T18:38:43.186505Z","iopub.status.idle":"2023-12-17T18:38:43.204553Z","shell.execute_reply.started":"2023-12-17T18:38:43.186469Z","shell.execute_reply":"2023-12-17T18:38:43.203344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.206027Z","iopub.execute_input":"2023-12-17T18:38:43.206881Z","iopub.status.idle":"2023-12-17T18:38:43.269408Z","shell.execute_reply.started":"2023-12-17T18:38:43.206846Z","shell.execute_reply":"2023-12-17T18:38:43.268106Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.27385Z","iopub.execute_input":"2023-12-17T18:38:43.274246Z","iopub.status.idle":"2023-12-17T18:38:43.29245Z","shell.execute_reply.started":"2023-12-17T18:38:43.274214Z","shell.execute_reply":"2023-12-17T18:38:43.290814Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.294479Z","iopub.execute_input":"2023-12-17T18:38:43.294866Z","iopub.status.idle":"2023-12-17T18:38:43.311283Z","shell.execute_reply.started":"2023-12-17T18:38:43.294819Z","shell.execute_reply":"2023-12-17T18:38:43.309946Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.313388Z","iopub.execute_input":"2023-12-17T18:38:43.313738Z","iopub.status.idle":"2023-12-17T18:38:43.360076Z","shell.execute_reply.started":"2023-12-17T18:38:43.313707Z","shell.execute_reply":"2023-12-17T18:38:43.358968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.361805Z","iopub.execute_input":"2023-12-17T18:38:43.362192Z","iopub.status.idle":"2023-12-17T18:38:43.384086Z","shell.execute_reply.started":"2023-12-17T18:38:43.362158Z","shell.execute_reply":"2023-12-17T18:38:43.382744Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.385878Z","iopub.execute_input":"2023-12-17T18:38:43.386784Z","iopub.status.idle":"2023-12-17T18:38:43.402718Z","shell.execute_reply.started":"2023-12-17T18:38:43.386733Z","shell.execute_reply":"2023-12-17T18:38:43.401355Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.describe(include = 'all')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.404156Z","iopub.execute_input":"2023-12-17T18:38:43.404488Z","iopub.status.idle":"2023-12-17T18:38:43.465925Z","shell.execute_reply.started":"2023-12-17T18:38:43.404456Z","shell.execute_reply":"2023-12-17T18:38:43.464745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.467259Z","iopub.execute_input":"2023-12-17T18:38:43.467592Z","iopub.status.idle":"2023-12-17T18:38:43.475102Z","shell.execute_reply.started":"2023-12-17T18:38:43.467563Z","shell.execute_reply":"2023-12-17T18:38:43.473755Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.477073Z","iopub.execute_input":"2023-12-17T18:38:43.477944Z","iopub.status.idle":"2023-12-17T18:38:43.48708Z","shell.execute_reply.started":"2023-12-17T18:38:43.47787Z","shell.execute_reply":"2023-12-17T18:38:43.486126Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.489316Z","iopub.execute_input":"2023-12-17T18:38:43.490202Z","iopub.status.idle":"2023-12-17T18:38:43.498302Z","shell.execute_reply.started":"2023-12-17T18:38:43.490143Z","shell.execute_reply":"2023-12-17T18:38:43.497147Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Handling missing values","metadata":{}},{"cell_type":"code","source":"# Checking for missing values\n# Missing values in each column \n\ntrain_missing = list(train_df.isnull().sum())\nval_missing = list(validation_df.isnull().sum())\ntest_misisng = list(test_df.isnull().sum())","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.510727Z","iopub.execute_input":"2023-12-17T18:38:43.511166Z","iopub.status.idle":"2023-12-17T18:38:43.537575Z","shell.execute_reply.started":"2023-12-17T18:38:43.511128Z","shell.execute_reply":"2023-12-17T18:38:43.536318Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the percentage of missing values\n# Percentage of missing values in each column\n\ntrain_missing_percent = list(train_df.isnull().sum() / len(train_df) * 100)\nval_missing_percent = list(validation_df.isnull().sum() / len(validation_df) * 100)\ntest_misisng_percent = list(test_df.isnull().sum() / len(test_df) * 100)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.539Z","iopub.execute_input":"2023-12-17T18:38:43.539391Z","iopub.status.idle":"2023-12-17T18:38:43.566861Z","shell.execute_reply.started":"2023-12-17T18:38:43.539356Z","shell.execute_reply":"2023-12-17T18:38:43.565844Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(train_df.isnull())\nplt.title(\"Missing values in training dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:43.56824Z","iopub.execute_input":"2023-12-17T18:38:43.568798Z","iopub.status.idle":"2023-12-17T18:38:45.424546Z","shell.execute_reply.started":"2023-12-17T18:38:43.568762Z","shell.execute_reply":"2023-12-17T18:38:45.423011Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(validation_df.isnull())\nplt.title(\"Missing values in validation dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:45.426415Z","iopub.execute_input":"2023-12-17T18:38:45.426931Z","iopub.status.idle":"2023-12-17T18:38:47.157012Z","shell.execute_reply.started":"2023-12-17T18:38:45.42686Z","shell.execute_reply":"2023-12-17T18:38:47.155658Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.displot(validation_df.isnull())\nplt.title(\"Missing values in test dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:47.158475Z","iopub.execute_input":"2023-12-17T18:38:47.15887Z","iopub.status.idle":"2023-12-17T18:38:48.860668Z","shell.execute_reply.started":"2023-12-17T18:38:47.158837Z","shell.execute_reply":"2023-12-17T18:38:48.859519Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(train_df.isnull(), cmap = 'viridis')\nplt.title(\"Missing values in training dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:48.862508Z","iopub.execute_input":"2023-12-17T18:38:48.863267Z","iopub.status.idle":"2023-12-17T18:38:49.632562Z","shell.execute_reply.started":"2023-12-17T18:38:48.863218Z","shell.execute_reply":"2023-12-17T18:38:49.631217Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(validation_df.isnull(), cmap = 'viridis')\nplt.title(\"Missing values in validation dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:49.634296Z","iopub.execute_input":"2023-12-17T18:38:49.634783Z","iopub.status.idle":"2023-12-17T18:38:50.411075Z","shell.execute_reply.started":"2023-12-17T18:38:49.634737Z","shell.execute_reply":"2023-12-17T18:38:50.409685Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.heatmap(validation_df.isnull(), cmap = 'viridis')\nplt.title(\"Missing values in test dataset\")\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:50.413085Z","iopub.execute_input":"2023-12-17T18:38:50.41361Z","iopub.status.idle":"2023-12-17T18:38:51.145603Z","shell.execute_reply.started":"2023-12-17T18:38:50.413559Z","shell.execute_reply":"2023-12-17T18:38:51.144295Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Missing values dataframe\n\nmissing_df = pd.DataFrame({'Columns' : list(train_df.columns), 'Train_missing' : train_missing, 'Percent_Train_missing' : train_missing_percent, \n                'Validation_missing' : val_missing, 'Percent_Val_missing' : val_missing_percent, 'Test_missing' : test_misisng, \n                           'Percent_Test_missing' : test_misisng_percent})\nmissing_df","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:51.147231Z","iopub.execute_input":"2023-12-17T18:38:51.147677Z","iopub.status.idle":"2023-12-17T18:38:51.169138Z","shell.execute_reply.started":"2023-12-17T18:38:51.147634Z","shell.execute_reply":"2023-12-17T18:38:51.167956Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see similar trend of missing data in bith training and validation dataframes.\n- But there alot more missing values in the test dataframe.\n- We can use the non null values in train and validation to determine the significance and values to treat the missing values in test dataframe.","metadata":{}},{"cell_type":"markdown","source":"#### Missing value treatment","metadata":{}},{"cell_type":"code","source":"# There are no missing values in Year column in train and validation dataframes, but there are missing year values in train dataframe\n# Filing the missing year values in test set based on values of training set\n# Converting the year column to a number datatype from object datatype\n\ntrain_df['Year'] = train_df['Year'].str.extract('([0-9]+)').astype(int)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:51.17078Z","iopub.execute_input":"2023-12-17T18:38:51.171283Z","iopub.status.idle":"2023-12-17T18:38:51.209412Z","shell.execute_reply.started":"2023-12-17T18:38:51.171233Z","shell.execute_reply":"2023-12-17T18:38:51.208208Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df['Year'] = validation_df['Year'].str.extract('([0-9]+)').astype(int)\nvalidation_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:51.210632Z","iopub.execute_input":"2023-12-17T18:38:51.210978Z","iopub.status.idle":"2023-12-17T18:38:51.236034Z","shell.execute_reply.started":"2023-12-17T18:38:51.210948Z","shell.execute_reply":"2023-12-17T18:38:51.234924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Year'] = test_df['Year'].str.replace(r'[()]', '', regex=True)\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:51.237719Z","iopub.execute_input":"2023-12-17T18:38:51.238215Z","iopub.status.idle":"2023-12-17T18:38:51.269919Z","shell.execute_reply.started":"2023-12-17T18:38:51.23818Z","shell.execute_reply":"2023-12-17T18:38:51.268954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train_df['Year'].values, label = \"Train\")\nsns.distplot(validation_df['Year'].values, label = \"Validation\")\nsns.distplot(test_df['Year'].values, label = \"Test\")\nplt.legend(['Train', 'Train', 'Validation', 'Validation', 'Test', 'Test'])\nplt.xlabel('Year')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:51.270938Z","iopub.execute_input":"2023-12-17T18:38:51.271273Z","iopub.status.idle":"2023-12-17T18:38:52.410318Z","shell.execute_reply.started":"2023-12-17T18:38:51.271242Z","shell.execute_reply":"2023-12-17T18:38:52.409112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see that the Train and validation is similarly distributed.\n- While the test dat frame with missing values has a very diiferent distribution.\n- We can impute the missing values from train into test.\n- Even tough the year is a number it is a categorical variable, so we have to fill with mode of the column.","metadata":{}},{"cell_type":"code","source":"mode_year = train_df['Year'].mode()\ntest_df['Year'] = test_df['Year'].fillna(2019)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:52.411559Z","iopub.execute_input":"2023-12-17T18:38:52.411954Z","iopub.status.idle":"2023-12-17T18:38:52.419954Z","shell.execute_reply.started":"2023-12-17T18:38:52.411921Z","shell.execute_reply":"2023-12-17T18:38:52.419091Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking why the column still shows null values\n\ntest_df['Year'].info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:52.421411Z","iopub.execute_input":"2023-12-17T18:38:52.422439Z","iopub.status.idle":"2023-12-17T18:38:52.440845Z","shell.execute_reply.started":"2023-12-17T18:38:52.422401Z","shell.execute_reply":"2023-12-17T18:38:52.439971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.distplot(train_df['Year'].values, label = \"Train\")\nsns.distplot(validation_df['Year'].values, label = \"Validation\")\nsns.distplot(test_df['Year'].values, label = \"Test\")\nplt.legend(['Train', 'Train', 'Validation', 'Validation', 'Test', 'Test'])\nplt.xlabel('Year')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:52.442024Z","iopub.execute_input":"2023-12-17T18:38:52.442768Z","iopub.status.idle":"2023-12-17T18:38:53.320568Z","shell.execute_reply.started":"2023-12-17T18:38:52.442728Z","shell.execute_reply":"2023-12-17T18:38:53.319303Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values in duration column\n\ntrain_df['Duration'] = train_df['Duration'].str.extract('([0-9]+)').astype(float)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:53.322217Z","iopub.execute_input":"2023-12-17T18:38:53.322708Z","iopub.status.idle":"2023-12-17T18:38:53.357616Z","shell.execute_reply.started":"2023-12-17T18:38:53.322672Z","shell.execute_reply":"2023-12-17T18:38:53.356417Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df['Duration'] = validation_df['Duration'].str.extract('([0-9]+)').astype(float)\nvalidation_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:53.359073Z","iopub.execute_input":"2023-12-17T18:38:53.359542Z","iopub.status.idle":"2023-12-17T18:38:53.384708Z","shell.execute_reply.started":"2023-12-17T18:38:53.359507Z","shell.execute_reply":"2023-12-17T18:38:53.383505Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Duration'] = test_df['Duration'].str.extract('([0-9]+)').astype(float)\ntest_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:53.386375Z","iopub.execute_input":"2023-12-17T18:38:53.386855Z","iopub.status.idle":"2023-12-17T18:38:53.422312Z","shell.execute_reply.started":"2023-12-17T18:38:53.386813Z","shell.execute_reply":"2023-12-17T18:38:53.420975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the distribution of duration\n\nsns.distplot(train_df['Duration'].values, label = \"Duration\")\nsns.distplot(validation_df['Duration'].values, label = \"Validation\")\nsns.distplot(test_df['Duration'].values, label = \"Test\")\nplt.legend(['Train', 'Train', 'Validation', 'Validation', 'Test', 'Test'])\nplt.xlabel('Duration')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:53.424156Z","iopub.execute_input":"2023-12-17T18:38:53.425629Z","iopub.status.idle":"2023-12-17T18:38:54.403291Z","shell.execute_reply.started":"2023-12-17T18:38:53.425585Z","shell.execute_reply":"2023-12-17T18:38:54.402059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We can see that the distribution of training and validation is similar while that of test is a bit off","metadata":{}},{"cell_type":"code","source":"# Checking fo outliers to see if we need to fill the missing values with mean or median\n\nplt.figure(figsize = (10, 5))\nplt.subplot(1, 2, 1)\nsns.boxplot(train_df['Duration'].values)\n\nplt.subplot(1, 2, 2)\nsns.boxenplot(train_df['Duration'].values)\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:54.404937Z","iopub.execute_input":"2023-12-17T18:38:54.405387Z","iopub.status.idle":"2023-12-17T18:38:54.79265Z","shell.execute_reply.started":"2023-12-17T18:38:54.405351Z","shell.execute_reply":"2023-12-17T18:38:54.791284Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see outliers in training dataset, so deciding to fill the null values with median of train.","metadata":{}},{"cell_type":"code","source":"# Filling the missing values\n\nmedian_duration = train_df['Duration'].median()\n\ntrain_df['Duration'] = train_df['Duration'].fillna(median_duration)\nvalidation_df['Duration'] = validation_df['Duration'].fillna(median_duration)\ntest_df['Duration'] = test_df['Duration'].fillna(median_duration)\n\ntrain_df['Duration'].info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:54.794156Z","iopub.execute_input":"2023-12-17T18:38:54.794551Z","iopub.status.idle":"2023-12-17T18:38:54.811442Z","shell.execute_reply.started":"2023-12-17T18:38:54.794514Z","shell.execute_reply":"2023-12-17T18:38:54.810097Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df['Duration'].info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:54.816241Z","iopub.execute_input":"2023-12-17T18:38:54.816633Z","iopub.status.idle":"2023-12-17T18:38:54.828575Z","shell.execute_reply.started":"2023-12-17T18:38:54.8166Z","shell.execute_reply":"2023-12-17T18:38:54.826119Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Duration'].info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:54.830772Z","iopub.execute_input":"2023-12-17T18:38:54.831162Z","iopub.status.idle":"2023-12-17T18:38:54.84164Z","shell.execute_reply.started":"2023-12-17T18:38:54.831127Z","shell.execute_reply":"2023-12-17T18:38:54.840334Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Seeing the distribution after filling the missing values.\n\nsns.distplot(train_df['Duration'].values, label = \"Duration\")\nsns.distplot(validation_df['Duration'].values, label = \"Validation\")\nsns.distplot(test_df['Duration'].values, label = \"Test\")\nplt.legend(['Train', 'Train', 'Validation', 'Validation', 'Test', 'Test'])\nplt.xlabel('Duration')\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:54.842975Z","iopub.execute_input":"2023-12-17T18:38:54.843498Z","iopub.status.idle":"2023-12-17T18:38:55.926137Z","shell.execute_reply.started":"2023-12-17T18:38:54.843465Z","shell.execute_reply":"2023-12-17T18:38:55.925079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values in genre column\n\ndef expand_genre(df):\n    genres_df = df['Genre'].str.split(', ', expand = True)\n    df = pd.concat([df, genres_df], axis = 1)\n    df.rename(columns = {0 : 'Genre_1', 1 : 'Genre_2', 2 : 'Genre_3'}, inplace = True)\n    df.drop('Genre', axis = 1, inplace = True)\n    return df","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:55.927979Z","iopub.execute_input":"2023-12-17T18:38:55.928654Z","iopub.status.idle":"2023-12-17T18:38:55.934918Z","shell.execute_reply.started":"2023-12-17T18:38:55.928613Z","shell.execute_reply":"2023-12-17T18:38:55.933984Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = expand_genre(train_df)\nvalidation_df = expand_genre(validation_df)\ntest_df = expand_genre(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:55.936454Z","iopub.execute_input":"2023-12-17T18:38:55.937073Z","iopub.status.idle":"2023-12-17T18:38:55.99864Z","shell.execute_reply.started":"2023-12-17T18:38:55.937031Z","shell.execute_reply":"2023-12-17T18:38:55.997139Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"Train Genre_1 missing :\", (train_df['Genre_1'].isnull().sum() / len(train_df['Genre_1']) * 100))\nprint(\"Train Genre_2 missing :\", (train_df['Genre_2'].isnull().sum() / len(train_df['Genre_2']) * 100))\nprint(\"Train Genre_3 missing :\", (train_df['Genre_3'].isnull().sum() / len(train_df['Genre_3']) * 100))\nprint(\"Validation Genre_1 missing :\", (validation_df['Genre_1'].isnull().sum() / len(validation_df['Genre_1']) * 100))\nprint(\"Validation Genre_2 missing :\", (validation_df['Genre_2'].isnull().sum() / len(validation_df['Genre_2']) * 100))\nprint(\"Validation Genre_3 missing :\", (validation_df['Genre_3'].isnull().sum() / len(validation_df['Genre_3']) * 100))\nprint(\"Test Genre_1 missing :\", (test_df['Genre_1'].isnull().sum() / len(test_df['Genre_1']) * 100))\nprint(\"Test Genre_2 missing :\", (test_df['Genre_2'].isnull().sum() / len(test_df['Genre_2']) * 100))\nprint(\"Test Genre_3 missing :\", (test_df['Genre_3'].isnull().sum() / len(test_df['Genre_3']) * 100))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.000322Z","iopub.execute_input":"2023-12-17T18:38:56.000695Z","iopub.status.idle":"2023-12-17T18:38:56.020726Z","shell.execute_reply.started":"2023-12-17T18:38:56.000661Z","shell.execute_reply":"2023-12-17T18:38:56.019509Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the missing values in the second and third sub-genre has missing values more than 35%, we drop the sub-genres","metadata":{}},{"cell_type":"code","source":"def drop_genre(df):\n    df.drop(['Genre_2','Genre_3'], axis = 1, inplace = True)\n    df.rename(columns = {'Genre_1' : 'Genre'}, inplace = True)\n    return df\n\ntrain_df = drop_genre(train_df)\nvalidation_df = drop_genre(validation_df)\ntest_df = drop_genre(test_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.022582Z","iopub.execute_input":"2023-12-17T18:38:56.023012Z","iopub.status.idle":"2023-12-17T18:38:56.047297Z","shell.execute_reply.started":"2023-12-17T18:38:56.022973Z","shell.execute_reply":"2023-12-17T18:38:56.046132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.048813Z","iopub.execute_input":"2023-12-17T18:38:56.049304Z","iopub.status.idle":"2023-12-17T18:38:56.070839Z","shell.execute_reply.started":"2023-12-17T18:38:56.049265Z","shell.execute_reply":"2023-12-17T18:38:56.069663Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.072329Z","iopub.execute_input":"2023-12-17T18:38:56.072676Z","iopub.status.idle":"2023-12-17T18:38:56.095367Z","shell.execute_reply.started":"2023-12-17T18:38:56.072645Z","shell.execute_reply":"2023-12-17T18:38:56.094053Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.097323Z","iopub.execute_input":"2023-12-17T18:38:56.097723Z","iopub.status.idle":"2023-12-17T18:38:56.117539Z","shell.execute_reply.started":"2023-12-17T18:38:56.097653Z","shell.execute_reply":"2023-12-17T18:38:56.11622Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values based on Mode Imputaion method \n\nmode_per_year = train_df.groupby('Year')['Genre'].apply(lambda x: x.mode().iloc[0])    # Gives a df with node of each year\ntrain_df['Genre'] = train_df.apply(lambda row: mode_per_year[row['Year']] if pd.isnull(row['Genre']) else row['Genre'], axis=1)\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.120109Z","iopub.execute_input":"2023-12-17T18:38:56.120976Z","iopub.status.idle":"2023-12-17T18:38:56.270462Z","shell.execute_reply.started":"2023-12-17T18:38:56.120928Z","shell.execute_reply":"2023-12-17T18:38:56.269251Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df['Genre'] = validation_df.apply(lambda row: mode_per_year[row['Year']] if pd.isnull(row['Genre']) else row['Genre'], axis=1)\nvalidation_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.271467Z","iopub.execute_input":"2023-12-17T18:38:56.27181Z","iopub.status.idle":"2023-12-17T18:38:56.327312Z","shell.execute_reply.started":"2023-12-17T18:38:56.271781Z","shell.execute_reply":"2023-12-17T18:38:56.325986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values of the genre in test df with the mode of train df\n\ngenre_mode = train_df['Genre'].mode()\ngenre_mode","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.328759Z","iopub.execute_input":"2023-12-17T18:38:56.329179Z","iopub.status.idle":"2023-12-17T18:38:56.339691Z","shell.execute_reply.started":"2023-12-17T18:38:56.329144Z","shell.execute_reply":"2023-12-17T18:38:56.338408Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Genre'] = test_df['Genre'].fillna('Drama')\ntest_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.341269Z","iopub.execute_input":"2023-12-17T18:38:56.341679Z","iopub.status.idle":"2023-12-17T18:38:56.369854Z","shell.execute_reply.started":"2023-12-17T18:38:56.341637Z","shell.execute_reply":"2023-12-17T18:38:56.368301Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Treating the missing values in Votes column in test dataset\n# Checking the unique values\n\nset(test_df['Votes'].tolist())","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.37166Z","iopub.execute_input":"2023-12-17T18:38:56.37214Z","iopub.status.idle":"2023-12-17T18:38:56.381796Z","shell.execute_reply.started":"2023-12-17T18:38:56.372095Z","shell.execute_reply":"2023-12-17T18:38:56.380504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# The one non null value seems to be a wrong value, making it to nan\n\ntest_df.index[test_df['Votes'] == '$5.16M']","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.382986Z","iopub.execute_input":"2023-12-17T18:38:56.383368Z","iopub.status.idle":"2023-12-17T18:38:56.39903Z","shell.execute_reply.started":"2023-12-17T18:38:56.383333Z","shell.execute_reply":"2023-12-17T18:38:56.398105Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df['Votes'][9500] = pd.NA","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.400249Z","iopub.execute_input":"2023-12-17T18:38:56.401266Z","iopub.status.idle":"2023-12-17T18:38:56.417144Z","shell.execute_reply.started":"2023-12-17T18:38:56.401226Z","shell.execute_reply":"2023-12-17T18:38:56.415854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.419352Z","iopub.execute_input":"2023-12-17T18:38:56.419776Z","iopub.status.idle":"2023-12-17T18:38:56.444876Z","shell.execute_reply.started":"2023-12-17T18:38:56.419741Z","shell.execute_reply":"2023-12-17T18:38:56.443626Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values in the names of Directors and actors as 'Not Available'\n\ndef fill_names(df):\n    df['Director'] = df['Director'].fillna('Not Available')\n    df['Actor 1'] = df['Actor 1'].fillna('Not Available')\n    df['Actor 2'] = df['Actor 2'].fillna('Not Available')\n    df['Actor 3'] = df['Actor 3'].fillna('Not Available')\n    return df\n\ntrain_df = fill_names(train_df)\nvalidation_df = fill_names(validation_df)\ntest_df = fill_names(test_df)\n\ntrain_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.446763Z","iopub.execute_input":"2023-12-17T18:38:56.447207Z","iopub.status.idle":"2023-12-17T18:38:56.486835Z","shell.execute_reply.started":"2023-12-17T18:38:56.447167Z","shell.execute_reply":"2023-12-17T18:38:56.485522Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.488477Z","iopub.execute_input":"2023-12-17T18:38:56.489401Z","iopub.status.idle":"2023-12-17T18:38:56.510038Z","shell.execute_reply.started":"2023-12-17T18:38:56.489355Z","shell.execute_reply":"2023-12-17T18:38:56.508835Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_df.info()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.512145Z","iopub.execute_input":"2023-12-17T18:38:56.512637Z","iopub.status.idle":"2023-12-17T18:38:56.535444Z","shell.execute_reply.started":"2023-12-17T18:38:56.512589Z","shell.execute_reply":"2023-12-17T18:38:56.534272Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for missing values\n\ntrain_missing = list(train_df.isnull().sum())\nval_missing = list(validation_df.isnull().sum())\ntest_misisng = list(test_df.isnull().sum())\n\ntrain_missing_percent = list(train_df.isnull().sum() / len(train_df) * 100)\nval_missing_percent = list(validation_df.isnull().sum() / len(validation_df) * 100)\ntest_misisng_percent = list(test_df.isnull().sum() / len(test_df) * 100)\n\nmissing_df = pd.DataFrame({'Columns' : list(train_df.columns), 'Train_missing' : train_missing, 'Percent_Train_missing' : train_missing_percent, \n                'Validation_missing' : val_missing, 'Percent_Val_missing' : val_missing_percent, 'Test_missing' : test_misisng, \n                           'Percent_Test_missing' : test_misisng_percent})\nmissing_df","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.537045Z","iopub.execute_input":"2023-12-17T18:38:56.538242Z","iopub.status.idle":"2023-12-17T18:38:56.606695Z","shell.execute_reply.started":"2023-12-17T18:38:56.538183Z","shell.execute_reply":"2023-12-17T18:38:56.605494Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"All the missing data is now cleared","metadata":{}},{"cell_type":"markdown","source":"### Exploratory Data Analysis","metadata":{}},{"cell_type":"code","source":"# Exploring the signficance of every variable\n\nfilled_df = pd.concat([train_df, validation_df], axis = 0)\nfilled_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.607976Z","iopub.execute_input":"2023-12-17T18:38:56.60872Z","iopub.status.idle":"2023-12-17T18:38:56.628693Z","shell.execute_reply.started":"2023-12-17T18:38:56.608683Z","shell.execute_reply":"2023-12-17T18:38:56.627567Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filled_df['Votes'].info","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.630553Z","iopub.execute_input":"2023-12-17T18:38:56.631169Z","iopub.status.idle":"2023-12-17T18:38:56.641208Z","shell.execute_reply.started":"2023-12-17T18:38:56.631134Z","shell.execute_reply":"2023-12-17T18:38:56.64015Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Votes should be a numberic column, but it is of object datatype, converting the column to numeric datatype\n\nfilled_df['Votes'] = filled_df['Votes'].str.replace(',','').astype(int)\ntrain_df['Votes'] = train_df['Votes'].str.replace(',','').astype(int)\nvalidation_df['Votes'] = validation_df['Votes'].str.replace(',','').astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.643525Z","iopub.execute_input":"2023-12-17T18:38:56.644061Z","iopub.status.idle":"2023-12-17T18:38:56.669025Z","shell.execute_reply.started":"2023-12-17T18:38:56.644013Z","shell.execute_reply":"2023-12-17T18:38:56.667991Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.scatterplot(x = filled_df['Rating'], y = filled_df['Votes'])\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:56.670389Z","iopub.execute_input":"2023-12-17T18:38:56.670912Z","iopub.status.idle":"2023-12-17T18:38:57.069997Z","shell.execute_reply.started":"2023-12-17T18:38:56.67085Z","shell.execute_reply":"2023-12-17T18:38:57.068778Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the data to understand the significance\n\nnumeric_cols = filled_df.select_dtypes(include = np.number)\ncol_names = list(numeric_cols.columns)\ncol_index = 0\nplt_rows = 2\nplt_cols = 2\n\nfig, ax = plt.subplots(nrows = plt_rows, ncols = plt_cols, figsize = (10, 10))\n\nfor row_count in range(plt_rows):\n    for col_count in range(plt_cols):\n        ax[row_count][col_count].scatter(x = filled_df[col_names[col_index]], y = filled_df['Rating'], c = ['b'])\n        ax[row_count][col_count].set_ylabel(col_names[col_index])\n        col_index += 1","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:57.071375Z","iopub.execute_input":"2023-12-17T18:38:57.071739Z","iopub.status.idle":"2023-12-17T18:38:58.57123Z","shell.execute_reply.started":"2023-12-17T18:38:57.071707Z","shell.execute_reply":"2023-12-17T18:38:58.56999Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the heat map\n\n# plt.figure(figsize = (4,4))\n# sns.heatmap(filled_df.corr(), annot = True, cmap = 'coolwarm')\n# plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:58.572626Z","iopub.execute_input":"2023-12-17T18:38:58.573048Z","iopub.status.idle":"2023-12-17T18:38:58.578387Z","shell.execute_reply.started":"2023-12-17T18:38:58.573011Z","shell.execute_reply":"2023-12-17T18:38:58.577181Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sns.pairplot(filled_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:38:58.580561Z","iopub.execute_input":"2023-12-17T18:38:58.581298Z","iopub.status.idle":"2023-12-17T18:39:36.129665Z","shell.execute_reply.started":"2023-12-17T18:38:58.581259Z","shell.execute_reply":"2023-12-17T18:39:36.128574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### There is nothing unsual with that we can see from the above visualizations","metadata":{}},{"cell_type":"code","source":"# Checking the distributions of each column\n\ncol_index = 0\n\nfig, ax = plt.subplots(nrows = plt_rows, ncols = plt_cols, figsize = (10, 10))\n\nfor row_count in range(plt_rows):\n    for col_count in range(plt_cols):\n        ax[row_count][col_count].hist(filled_df[col_names[col_index]])\n        ax[row_count][col_count].set_ylabel(col_names[col_index])\n        col_index += 1","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:36.131067Z","iopub.execute_input":"2023-12-17T18:39:36.131415Z","iopub.status.idle":"2023-12-17T18:39:37.593377Z","shell.execute_reply.started":"2023-12-17T18:39:36.131383Z","shell.execute_reply":"2023-12-17T18:39:37.592096Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Viz functions\n\ndef top_ten(col):\n    filled_df[col].value_counts().sort_values(ascending = False)[:10].plot(kind = \"bar\")\n    plt.title(\"Top Ten {}s\".format(col))\n    plt.xlabel(col)\n    plt.ylabel(\"Count\")\n    plt.show()\n    \ndef word_map(col):\n    text_data = ' '.join(filled_df[col])\n    wordcloud = WordCloud(width = 800, height = 400, background_color = 'black').generate(text_data)\n    plt.figure(figsize = (10,5))\n    plt.imshow(wordcloud, interpolation = 'bilinear')\n    plt.axis('off')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:37.595245Z","iopub.execute_input":"2023-12-17T18:39:37.595715Z","iopub.status.idle":"2023-12-17T18:39:37.605336Z","shell.execute_reply.started":"2023-12-17T18:39:37.59567Z","shell.execute_reply":"2023-12-17T18:39:37.604234Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Name\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:37.606929Z","iopub.execute_input":"2023-12-17T18:39:37.607627Z","iopub.status.idle":"2023-12-17T18:39:38.045227Z","shell.execute_reply.started":"2023-12-17T18:39:37.60759Z","shell.execute_reply":"2023-12-17T18:39:38.043917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Year\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:38.066931Z","iopub.execute_input":"2023-12-17T18:39:38.067357Z","iopub.status.idle":"2023-12-17T18:39:38.472193Z","shell.execute_reply.started":"2023-12-17T18:39:38.067324Z","shell.execute_reply":"2023-12-17T18:39:38.470923Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Director\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:38.474166Z","iopub.execute_input":"2023-12-17T18:39:38.474967Z","iopub.status.idle":"2023-12-17T18:39:38.88513Z","shell.execute_reply.started":"2023-12-17T18:39:38.474916Z","shell.execute_reply":"2023-12-17T18:39:38.884242Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Actor 1\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:38.886723Z","iopub.execute_input":"2023-12-17T18:39:38.887293Z","iopub.status.idle":"2023-12-17T18:39:39.327997Z","shell.execute_reply.started":"2023-12-17T18:39:38.887255Z","shell.execute_reply":"2023-12-17T18:39:39.325321Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Actor 2\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:39.329436Z","iopub.execute_input":"2023-12-17T18:39:39.329767Z","iopub.status.idle":"2023-12-17T18:39:39.775142Z","shell.execute_reply.started":"2023-12-17T18:39:39.329737Z","shell.execute_reply":"2023-12-17T18:39:39.773955Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Actor 3\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:39.776586Z","iopub.execute_input":"2023-12-17T18:39:39.776978Z","iopub.status.idle":"2023-12-17T18:39:40.202549Z","shell.execute_reply.started":"2023-12-17T18:39:39.776938Z","shell.execute_reply":"2023-12-17T18:39:40.201243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_ten(\"Genre\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:40.204678Z","iopub.execute_input":"2023-12-17T18:39:40.205279Z","iopub.status.idle":"2023-12-17T18:39:40.618622Z","shell.execute_reply.started":"2023-12-17T18:39:40.205244Z","shell.execute_reply":"2023-12-17T18:39:40.6174Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"filled_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:40.620289Z","iopub.execute_input":"2023-12-17T18:39:40.621717Z","iopub.status.idle":"2023-12-17T18:39:40.639323Z","shell.execute_reply.started":"2023-12-17T18:39:40.621664Z","shell.execute_reply":"2023-12-17T18:39:40.637968Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_map(\"Director\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:40.641153Z","iopub.execute_input":"2023-12-17T18:39:40.641627Z","iopub.status.idle":"2023-12-17T18:39:42.206275Z","shell.execute_reply.started":"2023-12-17T18:39:40.641583Z","shell.execute_reply":"2023-12-17T18:39:42.205001Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_map(\"Name\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:42.207799Z","iopub.execute_input":"2023-12-17T18:39:42.208193Z","iopub.status.idle":"2023-12-17T18:39:43.743065Z","shell.execute_reply.started":"2023-12-17T18:39:42.208158Z","shell.execute_reply":"2023-12-17T18:39:43.741039Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_map(\"Actor 1\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:43.74486Z","iopub.execute_input":"2023-12-17T18:39:43.745274Z","iopub.status.idle":"2023-12-17T18:39:45.218038Z","shell.execute_reply.started":"2023-12-17T18:39:43.745235Z","shell.execute_reply":"2023-12-17T18:39:45.216944Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_map(\"Actor 2\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:45.219594Z","iopub.execute_input":"2023-12-17T18:39:45.220259Z","iopub.status.idle":"2023-12-17T18:39:46.74376Z","shell.execute_reply.started":"2023-12-17T18:39:45.220215Z","shell.execute_reply":"2023-12-17T18:39:46.742475Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_map(\"Actor 3\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:46.745904Z","iopub.execute_input":"2023-12-17T18:39:46.746429Z","iopub.status.idle":"2023-12-17T18:39:48.222352Z","shell.execute_reply.started":"2023-12-17T18:39:46.746345Z","shell.execute_reply":"2023-12-17T18:39:48.221042Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"word_map(\"Genre\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:48.223708Z","iopub.execute_input":"2023-12-17T18:39:48.224126Z","iopub.status.idle":"2023-12-17T18:39:48.817415Z","shell.execute_reply.started":"2023-12-17T18:39:48.224084Z","shell.execute_reply":"2023-12-17T18:39:48.816152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The movie name \"Anjaam\" has been used in 5 movies.\n- The most number of movies were released in the year 2019.\n- Director \"Mahesh Bhatt\" has directed the most number of movies.\n- \"Jeetendra\" has been the lead actor in most of the movies.\n- But we can see that \"Mithun Chakraborty\" is in the top 10 list of all actor categories, making him the most predominant actor.\n- \"Rehka\" has been the lead actress throughout.\n- The movie genres Drama and Action are far ahead of other genres.","metadata":{}},{"cell_type":"code","source":"# Checking each variable against the target variable\n\ndef rating_per(col, color):\n    avg_rating = filled_df.groupby(col)['Rating'].mean().reset_index()\n    plt.figure(figsize = (10, 6))\n    plt.bar(avg_rating[col], avg_rating['Rating'], color = color)\n    plt.title('{}-wise Ratings'.format(col))\n    plt.xlabel(col)\n    plt.ylabel('Rating')\n    plt.grid(axis='y', linestyle='--', alpha=0.7)\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:48.81878Z","iopub.execute_input":"2023-12-17T18:39:48.819256Z","iopub.status.idle":"2023-12-17T18:39:48.828452Z","shell.execute_reply.started":"2023-12-17T18:39:48.819223Z","shell.execute_reply":"2023-12-17T18:39:48.826703Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rating_per('Year', 'skyblue')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:48.830391Z","iopub.execute_input":"2023-12-17T18:39:48.830835Z","iopub.status.idle":"2023-12-17T18:39:49.421747Z","shell.execute_reply.started":"2023-12-17T18:39:48.830798Z","shell.execute_reply":"2023-12-17T18:39:49.420547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rating_per('Duration', 'black')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:49.42373Z","iopub.execute_input":"2023-12-17T18:39:49.424248Z","iopub.status.idle":"2023-12-17T18:39:50.113497Z","shell.execute_reply.started":"2023-12-17T18:39:49.4242Z","shell.execute_reply":"2023-12-17T18:39:50.112498Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"rating_per('Votes', 'white')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:50.115142Z","iopub.execute_input":"2023-12-17T18:39:50.115822Z","iopub.status.idle":"2023-12-17T18:39:54.450389Z","shell.execute_reply.started":"2023-12-17T18:39:50.115784Z","shell.execute_reply":"2023-12-17T18:39:54.449297Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def top_rated(col):\n    avg_rating = filled_df.groupby(col)['Rating'].mean().reset_index()\n    top20 = avg_rating.sort_values(by='Rating', ascending=False).head(20)[col]\n    top20_df = filled_df[filled_df[col].isin(top20)]\n    sorted_top20_df = top20_df.sort_values(by = 'Rating', ascending = False)\n    plt.figure(figsize = (12, 8))\n    sns.violinplot(x = 'Rating', y = col, data = sorted_top20_df, palette = 'muted')\n    plt.title('{}-wise Rating Distribution'.format(col))\n    plt.xlabel(col)\n    plt.ylabel('Rating')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:54.452101Z","iopub.execute_input":"2023-12-17T18:39:54.452778Z","iopub.status.idle":"2023-12-17T18:39:54.460474Z","shell.execute_reply.started":"2023-12-17T18:39:54.45274Z","shell.execute_reply":"2023-12-17T18:39:54.459214Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_rated('Director')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:54.462056Z","iopub.execute_input":"2023-12-17T18:39:54.46329Z","iopub.status.idle":"2023-12-17T18:39:55.018785Z","shell.execute_reply.started":"2023-12-17T18:39:54.463236Z","shell.execute_reply":"2023-12-17T18:39:55.017582Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_rated('Actor 1')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:55.020863Z","iopub.execute_input":"2023-12-17T18:39:55.021733Z","iopub.status.idle":"2023-12-17T18:39:55.594148Z","shell.execute_reply.started":"2023-12-17T18:39:55.021684Z","shell.execute_reply":"2023-12-17T18:39:55.592568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_rated('Actor 2')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:55.595368Z","iopub.execute_input":"2023-12-17T18:39:55.595744Z","iopub.status.idle":"2023-12-17T18:39:56.157804Z","shell.execute_reply.started":"2023-12-17T18:39:55.595712Z","shell.execute_reply":"2023-12-17T18:39:56.156467Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_rated('Actor 3')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:56.159152Z","iopub.execute_input":"2023-12-17T18:39:56.159491Z","iopub.status.idle":"2023-12-17T18:39:56.730801Z","shell.execute_reply.started":"2023-12-17T18:39:56.159461Z","shell.execute_reply":"2023-12-17T18:39:56.729521Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"top_rated('Genre')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:56.73306Z","iopub.execute_input":"2023-12-17T18:39:56.73355Z","iopub.status.idle":"2023-12-17T18:39:57.790667Z","shell.execute_reply.started":"2023-12-17T18:39:56.733501Z","shell.execute_reply":"2023-12-17T18:39:57.789537Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Checking the average rating per year, we see that the movies released between 1980 to 2010 are prety low and the trend is slowly changing as there is consistent increase in average ratingsa after 2000. \n- This may relate to the technological developments at the end of 20th Century, and how Indian cinema adapted to these technologies over the turn of the century.\n- Surprisingly we can see that the movies with duration around the median time all have lower ratings compared to the ones at the either end.\n- We see very dense votes under 10,000, making it difficult to come to any conclusions based of the number of votes.\n- Seeing the top 20 rated Directos and ACtors, we see that the highest rated doesn't have much of works under their name.\n- Simply saying the top rated Actors or Directors have very worked on very less number of movies.\n- Obviously the average rating of Drama genre is the highest this may be just because of the sheer number of drama movies.\n- Music genre having the next best ratings is quite surprising as compared to Family and Romance movies that are popular in India.","metadata":{}},{"cell_type":"code","source":"top_movies_overall = filled_df.nlargest(10, 'Rating')\n\n# Find the top 10 movies per year\ntop_movies_per_year = filled_df.groupby('Year').apply(lambda x: x.nlargest(10, 'Rating')).reset_index(drop=True)\n\n# Display the results\nprint(\"Top 10 Movies Overall:\")\nprint(top_movies_overall)\n\nprint(\"\\nTop 10 Movies Per Year:\")\nprint(top_movies_per_year)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:57.792271Z","iopub.execute_input":"2023-12-17T18:39:57.793075Z","iopub.status.idle":"2023-12-17T18:39:58.039166Z","shell.execute_reply.started":"2023-12-17T18:39:57.793039Z","shell.execute_reply":"2023-12-17T18:39:58.037841Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualising the top rated movies per year will not be clear because of the large number of rows\n# Trying to visualise the overall top rated movies based on the number of votes\n\nplt.figure(figsize=(12, 6))\nsns.scatterplot(x = 'Year', y = 'Rating', size = 'Votes', data = top_movies_overall, hue = 'Name', sizes = (50, 500), palette = 'viridis', alpha = 0.7)\nplt.title('Top 10 Movies Overall (Bubble Chart)')\nplt.xlabel('Year')\nplt.ylabel('Rating')\nplt.legend(bbox_to_anchor = (1, 1), loc = 'upper left', title = 'Movie Title')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:58.040817Z","iopub.execute_input":"2023-12-17T18:39:58.041356Z","iopub.status.idle":"2023-12-17T18:39:59.03741Z","shell.execute_reply.started":"2023-12-17T18:39:58.041308Z","shell.execute_reply":"2023-12-17T18:39:59.03572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see that the top rated movies overall have very low votes compared to the other mvies with thousands of votes.\n- We also see that the years 2020 and 2021 have the most number of top rated movies 7 of top 10\n- We also see that the all of the top rated movies are after 2017","metadata":{}},{"cell_type":"code","source":"# Checking the number of popular movies each year\n# We need to define what is popular\n# We can see popular as the movies with a certain rating or above, but from the above explorations\n# We know that some years had movies with low ratings\n# Considering the mean rating of each year as the threshold for populariry\n# We also cannot neglect the number of votes as even low rated movies can ve very polpular\n# Decided to check movies based or rating and based on votes with same metrics of mean per year as threshold\n\nmean_rating_per_year = filled_df.groupby('Year')['Rating'].mean().reset_index()\n\n# Merge the mean popularity back into the original dataframe\nmean_rated_df = pd.merge(filled_df, mean_rating_per_year, on = 'Year', suffixes=('', '_mean'))\n\n# Use the mean popularity as the threshold\nmean_rated_df['IsPopular_Rating'] = mean_rated_df['Rating'] > mean_rated_df['Rating_mean']\n\n# Count the number of popular movies released each year\npopular_rated_movies_count = mean_rated_df[mean_rated_df['IsPopular_Rating']].groupby('Year').size().reset_index(name = 'Number of Popular Movies (Rated)')\n\n# Visualize the number of popular movies released each year using a line chart\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='Year', y='Number of Popular Movies (Rated)', data=popular_rated_movies_count, marker='o', color='b')\nplt.title('Number of Popular Movies Released Each Year (Threshold: Mean Rating per year)')\nplt.xlabel('Year')\nplt.ylabel('Number of Popular Movies')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:59.038991Z","iopub.execute_input":"2023-12-17T18:39:59.039421Z","iopub.status.idle":"2023-12-17T18:39:59.506586Z","shell.execute_reply.started":"2023-12-17T18:39:59.039385Z","shell.execute_reply":"2023-12-17T18:39:59.505254Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(popular_rated_movies_count)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:59.508159Z","iopub.execute_input":"2023-12-17T18:39:59.508516Z","iopub.status.idle":"2023-12-17T18:39:59.517684Z","shell.execute_reply.started":"2023-12-17T18:39:59.50848Z","shell.execute_reply":"2023-12-17T18:39:59.516291Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mean_votes_per_year = filled_df.groupby('Year')['Votes'].mean().reset_index()\n\n# Merge the mean popularity back into the original dataframe\nmean_voted_df = pd.merge(filled_df, mean_votes_per_year, on = 'Year', suffixes=('', '_mean'))\n\n# Use the mean popularity as the threshold\nmean_voted_df['IsPopular_Voting'] = mean_voted_df['Votes'] > mean_voted_df['Votes_mean']\n\n# Count the number of popular movies released each year\npopular_voted_movies_count = mean_voted_df[mean_voted_df['IsPopular_Voting']].groupby('Year').size().reset_index(name = 'Number of Popular Movies (Voted)')\n\n# Visualize the number of popular movies released each year using a line chart\nplt.figure(figsize=(12, 6))\nsns.lineplot(x='Year', y='Number of Popular Movies (Voted)', data=popular_voted_movies_count, marker='o', color='r')\nplt.title('Number of Popular Movies Released Each Year (Threshold: Mean Voting per year)')\nplt.xlabel('Year')\nplt.ylabel('Number of Popular Movies')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:59.519874Z","iopub.execute_input":"2023-12-17T18:39:59.52042Z","iopub.status.idle":"2023-12-17T18:39:59.978393Z","shell.execute_reply.started":"2023-12-17T18:39:59.520364Z","shell.execute_reply":"2023-12-17T18:39:59.977114Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(12, 6))\nsns.lineplot(x='Year', y='Number of Popular Movies (Voted)', data=popular_voted_movies_count, marker='o', color='r')\nsns.lineplot(x='Year', y='Number of Popular Movies (Rated)', data=popular_rated_movies_count, marker='o', color='b')\nplt.title('Number of Popular Movies Released Each Year (Threshold: Mean Rating/Voting per year)')\nplt.xlabel('Year')\nplt.ylabel('Number of Popular Movies')\nplt.grid(True)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:39:59.980044Z","iopub.execute_input":"2023-12-17T18:39:59.980451Z","iopub.status.idle":"2023-12-17T18:40:00.410316Z","shell.execute_reply.started":"2023-12-17T18:39:59.980414Z","shell.execute_reply":"2023-12-17T18:40:00.408748Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that there is increase in the number of popular movies over the years and specially after the year 2000 we see a considerable increase compared to the previous years.","metadata":{}},{"cell_type":"code","source":"# Find the maximum rating for each year\nmax_rating_per_year = filled_df.groupby('Year')['Rating'].max().reset_index()\n\n# Merge the maximum rating back into the original dataframe\nmax_rated_df = pd.merge(filled_df, max_rating_per_year, on='Year', suffixes=('', '_max'))\n\n# Count the number of votes for movies that performed better in rating each year\nbetter_movies_votes_per_year = max_rated_df[max_rated_df['Rating'] == max_rated_df['Rating_max']].groupby('Year')['Votes'].sum().reset_index(name = 'Total Votes').sort_values(by = 'Year')\n\n# Count the number of votes for movies that performed better in rating overall\nbetter_movies_votes_overall = max_rated_df[max_rated_df['Rating'] == max_rated_df['Rating_max']].groupby('Name')['Votes'].sum().reset_index(name = 'Total Votes').sort_values(by = 'Total Votes', ascending = False)\n\nprint(\"Number of votes for movies that performed better in rating each year:\")\nprint(better_movies_votes_per_year)\n\nprint(\"\\nNumber of votes for movies that performed better in rating overall:\")\nprint(better_movies_votes_overall)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:00.411866Z","iopub.execute_input":"2023-12-17T18:40:00.412269Z","iopub.status.idle":"2023-12-17T18:40:00.451739Z","shell.execute_reply.started":"2023-12-17T18:40:00.412235Z","shell.execute_reply":"2023-12-17T18:40:00.450344Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the number of votes for movies that performed better in rating per year using a bar plot\n\nplt.figure(figsize = (12, 6))\nsns.lineplot(x = 'Year', y = 'Total Votes', data = better_movies_votes_per_year, color = 'green')\nplt.title('Number of Votes for Top-Rated Movies per Year')\nplt.ylabel('Total Votes')\nplt.xlabel('Year')\nplt.xlim(left = min(filled_df['Year']), right = 2025)\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:00.453459Z","iopub.execute_input":"2023-12-17T18:40:00.453969Z","iopub.status.idle":"2023-12-17T18:40:00.905522Z","shell.execute_reply.started":"2023-12-17T18:40:00.453917Z","shell.execute_reply":"2023-12-17T18:40:00.90382Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Visualize the number of votes for movies that performed better in rating overall using a bar plot\n\nplt.figure(figsize = (12, 20))\nsns.barplot(y = 'Name', x = 'Total Votes', data = better_movies_votes_overall, palette = 'viridis')\nplt.title('Number of Votes for Top-Rated Movies Overall')\nplt.ylabel('Movie Title')\nplt.xlabel('Total Votes')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:00.907297Z","iopub.execute_input":"2023-12-17T18:40:00.907678Z","iopub.status.idle":"2023-12-17T18:40:02.630319Z","shell.execute_reply.started":"2023-12-17T18:40:00.907646Z","shell.execute_reply":"2023-12-17T18:40:02.62911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see that 3 idiots is by far the mosed voted best rated movie overall in Indian cinema","metadata":{}},{"cell_type":"markdown","source":"### Futher Preprocesing and model building","metadata":{}},{"cell_type":"markdown","source":"#### Encoding all the categorical values before building the models.\n    - If there are alot of unique values, its better to drop the variable, encoding it makes the process more complex and insignificant\n    - If there are a few objects, proceed with encoding\n    - If there are only two unique values use Label encoder\n    - If there are more than 2 unique values use ONE HOT encoder\n    - ","metadata":{}},{"cell_type":"code","source":"# Encoding all the object datatype variables\n\nobj_list = train_df.select_dtypes(include = ['object']).columns.tolist()\nprint(obj_list)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.632149Z","iopub.execute_input":"2023-12-17T18:40:02.632546Z","iopub.status.idle":"2023-12-17T18:40:02.643308Z","shell.execute_reply.started":"2023-12-17T18:40:02.632504Z","shell.execute_reply":"2023-12-17T18:40:02.641737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the unique values in each object column to decide if we need to use them or not\n\nfor i in obj_list:\n    print(\"Unique values in training dataset for %s column is: %s\" % (i, train_df[i].nunique()))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.644824Z","iopub.execute_input":"2023-12-17T18:40:02.645238Z","iopub.status.idle":"2023-12-17T18:40:02.662793Z","shell.execute_reply.started":"2023-12-17T18:40:02.645204Z","shell.execute_reply":"2023-12-17T18:40:02.661488Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the percantage of unique values in each object column to decide if we need to use them or not\n\nfor i in obj_list:\n    print(\"Percentage of unique values in training dataset for %s column is: %s percent\" % (i, train_df[i].nunique() / len(train_df) * 100))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.664023Z","iopub.execute_input":"2023-12-17T18:40:02.664397Z","iopub.status.idle":"2023-12-17T18:40:02.683024Z","shell.execute_reply.started":"2023-12-17T18:40:02.664352Z","shell.execute_reply":"2023-12-17T18:40:02.681562Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.684687Z","iopub.execute_input":"2023-12-17T18:40:02.685292Z","iopub.status.idle":"2023-12-17T18:40:02.703901Z","shell.execute_reply.started":"2023-12-17T18:40:02.685247Z","shell.execute_reply":"2023-12-17T18:40:02.702367Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping the insignificant variables\n\ntrain_df = train_df.drop(['Name'], axis = 1)\nvalidation_df = validation_df.drop(['Name'], axis = 1)\n\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.705936Z","iopub.execute_input":"2023-12-17T18:40:02.706426Z","iopub.status.idle":"2023-12-17T18:40:02.731213Z","shell.execute_reply.started":"2023-12-17T18:40:02.70638Z","shell.execute_reply":"2023-12-17T18:40:02.72988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.73269Z","iopub.execute_input":"2023-12-17T18:40:02.733097Z","iopub.status.idle":"2023-12-17T18:40:02.753386Z","shell.execute_reply.started":"2023-12-17T18:40:02.733063Z","shell.execute_reply":"2023-12-17T18:40:02.752177Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Target encoding to encode the categorical variables\n# As there are a diverese range of categorical variables\n\ncategorical_variables = train_df.select_dtypes(include = ['object']).columns.tolist()\nencoder = ce.TargetEncoder(cols = categorical_variables)\nencoded_df = encoder.fit_transform(train_df[categorical_variables], train_df['Rating'])","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.755962Z","iopub.execute_input":"2023-12-17T18:40:02.756489Z","iopub.status.idle":"2023-12-17T18:40:02.896694Z","shell.execute_reply.started":"2023-12-17T18:40:02.756444Z","shell.execute_reply":"2023-12-17T18:40:02.895415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"encoded_df.columns = [f\"{col}_TargetEncoded\" for col in categorical_variables]\ntrain_df = pd.concat([train_df, encoded_df], axis = 1)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.898276Z","iopub.execute_input":"2023-12-17T18:40:02.898672Z","iopub.status.idle":"2023-12-17T18:40:02.922274Z","shell.execute_reply.started":"2023-12-17T18:40:02.898636Z","shell.execute_reply":"2023-12-17T18:40:02.921314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping the categorical variables and keeping the encoded columns\n\ntrain_df = train_df.drop(['Director', 'Actor 1', 'Actor 2', 'Actor 3', 'Genre'], axis = 1)\ntrain_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.923469Z","iopub.execute_input":"2023-12-17T18:40:02.924312Z","iopub.status.idle":"2023-12-17T18:40:02.943479Z","shell.execute_reply.started":"2023-12-17T18:40:02.924275Z","shell.execute_reply":"2023-12-17T18:40:02.942056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Traget encoding the validation df\n\nencoded_val_df = encoder.fit_transform(validation_df[categorical_variables], validation_df['Rating'])\nencoded_val_df.columns = [f\"{col}_TargetEncoded\" for col in categorical_variables]\nvalidation_df = pd.concat([validation_df, encoded_val_df], axis = 1)\nvalidation_df = validation_df.drop(['Director', 'Actor 1', 'Actor 2', 'Actor 3', 'Genre'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:02.945055Z","iopub.execute_input":"2023-12-17T18:40:02.945451Z","iopub.status.idle":"2023-12-17T18:40:03.044816Z","shell.execute_reply.started":"2023-12-17T18:40:02.945413Z","shell.execute_reply":"2023-12-17T18:40:03.042972Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:03.046648Z","iopub.execute_input":"2023-12-17T18:40:03.047051Z","iopub.status.idle":"2023-12-17T18:40:03.064762Z","shell.execute_reply.started":"2023-12-17T18:40:03.047016Z","shell.execute_reply":"2023-12-17T18:40:03.063351Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Now there are no variables of object datatype in our dataframe, now we can feed it to the model","metadata":{}},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:03.066678Z","iopub.execute_input":"2023-12-17T18:40:03.067807Z","iopub.status.idle":"2023-12-17T18:40:03.077426Z","shell.execute_reply.started":"2023-12-17T18:40:03.067764Z","shell.execute_reply":"2023-12-17T18:40:03.075584Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"validation_df.shape","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:03.079122Z","iopub.execute_input":"2023-12-17T18:40:03.079649Z","iopub.status.idle":"2023-12-17T18:40:03.093266Z","shell.execute_reply.started":"2023-12-17T18:40:03.07961Z","shell.execute_reply":"2023-12-17T18:40:03.09182Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"We see that the number of columns in validation df is lesser than that in the training df because some of genres in trainng dataset is not present in the validation dataset","metadata":{}},{"cell_type":"code","source":"# Exploratory data analysis\n# Exploring the signficance of every variable\n# Visualising the data to understand the significance\n# Create a scatter plot with points colored by the 'Rating' column\n\nnumeric_cols = train_df.select_dtypes(include = np.number)\ncol_names = list(numeric_cols.columns)\ncol_index = 0\nplt_rows = 3\nplt_cols = 3\n\nfig, ax = plt.subplots(nrows = plt_rows, ncols = plt_cols, figsize = (20, 20))\nax = ax.flatten()\n\nfor i, ax in enumerate(ax):\n#     for col_count in range(plt_cols):\n#         ax[row_count][col_count].scatterplot(y = train_df[col_names[col_index]], x = train_df.index, hue = \"Rating\")\n#         ax[row_count][col_count].set_ylabel(col_names[col_index])\n#         col_index += 1\n    sns.scatterplot(x = train_df[col_names[col_index]], y = train_df['Rating'], data = train_df, hue = \"Rating\", ax = ax)\n    ax.set_title(f'Subplot {i+1}')\n    col_index += 1\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:03.095351Z","iopub.execute_input":"2023-12-17T18:40:03.095922Z","iopub.status.idle":"2023-12-17T18:40:12.931374Z","shell.execute_reply.started":"2023-12-17T18:40:03.095798Z","shell.execute_reply":"2023-12-17T18:40:12.929112Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the heat map\n\nplt.figure(figsize = (16,20))\nsns.heatmap(train_df.corr(), annot = True, cmap = 'coolwarm')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:12.932848Z","iopub.execute_input":"2023-12-17T18:40:12.933258Z","iopub.status.idle":"2023-12-17T18:40:13.775961Z","shell.execute_reply.started":"2023-12-17T18:40:12.933225Z","shell.execute_reply":"2023-12-17T18:40:13.775049Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the pairplot\n\nsns.pairplot(train_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:40:13.777328Z","iopub.execute_input":"2023-12-17T18:40:13.778525Z","iopub.status.idle":"2023-12-17T18:41:06.71361Z","shell.execute_reply.started":"2023-12-17T18:40:13.778484Z","shell.execute_reply":"2023-12-17T18:41:06.711915Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- There is not much high correlation or auto correlation\n- Duration has very low correlation with Rating and Votes is also pretty low\n- We can proceed with the dataset for model building","metadata":{}},{"cell_type":"code","source":"# Splitting dependent and independent variable\n\nraw_x_train = train_df.drop(['Rating'], axis = 1)\nraw_y_train = train_df['Rating']\n\nraw_x_val = validation_df.drop(['Rating'], axis = 1)\nraw_y_val = validation_df['Rating']\n\nraw_x_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:06.715214Z","iopub.execute_input":"2023-12-17T18:41:06.715593Z","iopub.status.idle":"2023-12-17T18:41:06.7362Z","shell.execute_reply.started":"2023-12-17T18:41:06.715558Z","shell.execute_reply":"2023-12-17T18:41:06.735059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"raw_x_val.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:06.738105Z","iopub.execute_input":"2023-12-17T18:41:06.738532Z","iopub.status.idle":"2023-12-17T18:41:06.763261Z","shell.execute_reply.started":"2023-12-17T18:41:06.738493Z","shell.execute_reply":"2023-12-17T18:41:06.76199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the models\n\n# Linear Regression model\nlinear_model_raw = LinearRegression()\nlinear_model_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_lr = linear_model_raw.predict(raw_x_train)\nraw_y_pred_val_lr = linear_model_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Linear Regression model on raw data\")\nraw_lr_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_lr)\nraw_train_lr_r2s = r2_score(raw_y_train, raw_y_pred_train_lr)\nraw_val_lr_r2s = r2_score(raw_y_val, raw_y_pred_val_lr)\nprint(\"Mean Squared Error :\", raw_lr_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_lr_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_lr_r2s)\nprint(\"**************\" * 7)\n\n# Lasso Regression (L1 Regularization) going with alpha = 0.05, after checking various values\nlasso_model_raw = Lasso(alpha = 0.05)\nlasso_model_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_lar = lasso_model_raw.predict(raw_x_train)\nraw_y_pred_val_lar = lasso_model_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Lasso Regression (L1 Regularization) model on raw data\")\nraw_lar_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_lar)\nraw_train_lar_r2s = r2_score(raw_y_train, raw_y_pred_train_lar)\nraw_val_lar_r2s = r2_score(raw_y_val, raw_y_pred_val_lar)\nprint(\"Mean Squared Error :\", raw_lar_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_lar_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_lar_r2s)\nprint(\"**************\" * 7)\n\n# Ridge Regression (L2 Regularization) going with alpha = 0.05, after checking various values\nridge_model_raw = Ridge(alpha = 0.05)\nridge_model_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_ridge = ridge_model_raw.predict(raw_x_train)\nraw_y_pred_val_ridge = ridge_model_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Ridge Regression (L2 Regularization) model on raw data\")\nraw_ridge_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_ridge)\nraw_train_ridge_r2s = r2_score(raw_y_train, raw_y_pred_train_ridge)\nraw_val_ridge_r2s = r2_score(raw_y_val, raw_y_pred_val_ridge)\nprint(\"Mean Squared Error :\", raw_ridge_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_ridge_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_ridge_r2s)\nprint(\"**************\" * 7)\n\n# Elastic Net Regression (L1 and L2 Regularizations) going with alpha = 0.05, after checking various values\nenet_model_raw = ElasticNet(alpha = 0.05, random_state = 101)\nenet_model_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_enet = enet_model_raw.predict(raw_x_train)\nraw_y_pred_val_enet = enet_model_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Elastic Net Regression (L1 and L2 Regularizations) model on raw data\")\nraw_enet_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_enet)\nraw_train_enet_r2s = r2_score(raw_y_train, raw_y_pred_train_enet)\nraw_val_enet_r2s = r2_score(raw_y_val, raw_y_pred_val_enet)\nprint(\"Mean Squared Error :\", raw_enet_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_enet_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_enet_r2s)\nprint(\"**************\" * 7)\n\n# Decission Tree regression - Choosing max depth as 6 after trying different values\ndtree_raw = DecisionTreeRegressor(max_depth = 6)\ndtree_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_dtree = dtree_raw.predict(raw_x_train)\nraw_y_pred_val_dtree = dtree_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Decision Tree model on raw data\")\nraw_dtree_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_dtree)\nraw_train_dtree_r2s = r2_score(raw_y_train, raw_y_pred_train_dtree)\nraw_val_dtree_r2s = r2_score(raw_y_val, raw_y_pred_val_dtree)\nprint(\"Mean Squared Error :\", raw_dtree_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_dtree_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_dtree_r2s)\nprint(\"**************\" * 7)\n\n# Random Forest regression - Choosing max depth as 6 after trying different values\n# Choosing the parameters after trying different values\nrf_raw = RandomForestRegressor(n_estimators = 100, random_state = 1, max_depth = 6)\nrf_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_rf = rf_raw.predict(raw_x_train)\nraw_y_pred_val_rf = rf_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Random Forest model on raw data\")\nraw_rf_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_rf)\nraw_train_rf_r2s = r2_score(raw_y_train, raw_y_pred_train_rf)\nraw_val_rf_r2s = r2_score(raw_y_val, raw_y_pred_val_rf)\nprint(\"Mean Squared Error :\", raw_rf_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_rf_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_rf_r2s)\nprint(\"**************\" * 7)\n\n# Gradient Boosting Regression model\ngb_raw = GradientBoostingRegressor(n_estimators = 100, max_depth = 3, random_state = 123)\ngb_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_gb = gb_raw.predict(raw_x_train)\nraw_y_pred_val_gb = gb_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for Gradient Boost Regressor model on raw data\")\nraw_gb_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_gb)\nraw_train_gb_r2s = r2_score(raw_y_train, raw_y_pred_train_gb)\nraw_val_gb_r2s = r2_score(raw_y_val, raw_y_pred_val_gb)\nprint(\"Mean Squared Error :\", raw_gb_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_gb_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_gb_r2s)\nprint(\"**************\" * 7)\n\n# LGBoost Regression model - Max depth and Num_leaves = 3 after testing various values\nlgb_raw = lgb.LGBMRegressor(random_state = 11, max_depth = 3, num_leaves = 3, force_col_wise = True)\nlgb_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_lgb = lgb_raw.predict(raw_x_train)\nraw_y_pred_val_lgb = lgb_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for LGBoost model on raw data\")\nraw_lgb_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_lgb)\nraw_train_lgb_r2s = r2_score(raw_y_train, raw_y_pred_train_lgb)\nraw_val_lgb_r2s = r2_score(raw_y_val, raw_y_pred_val_lgb)\nprint(\"Mean Squared Error :\", raw_lgb_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_lgb_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_lgb_r2s)\nprint(\"**************\" * 7)\n\n# XGBoost Regression model - Max depth = 2 after testing various values\nxgb_raw = xgb.XGBRegressor(random_state = 111, max_depth = 2)\nxgb_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_xgb = xgb_raw.predict(raw_x_train)\nraw_y_pred_val_xgb = xgb_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for XGBoost model on raw data\")\nraw_xgb_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_xgb)\nraw_train_xgb_r2s = r2_score(raw_y_train, raw_y_pred_train_xgb)\nraw_val_xgb_r2s = r2_score(raw_y_val, raw_y_pred_val_xgb)\nprint(\"Mean Squared Error :\", raw_xgb_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_xgb_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_xgb_r2s)\nprint(\"**************\" * 7)\n\n# # Support Vector Regression model - Linear kernel\n# SVM turns out to be computationally very expensive\n# svr_linear_raw = SVR(kernel = 'linear')\n# svr_linear_raw.fit(raw_x_train, raw_y_train)\n# raw_y_pred_train_svr_linear = svr_linear_raw.predict(raw_x_train)\n# raw_y_pred_val_svr_linear = svr_linear_raw.predict(raw_x_val)\n\n# print(\"Accuracy Scores for Support Vector Model with linear kernel on raw data\")\n# raw_svr_linear_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_svr_linear)\n# raw_train_svr_linear_r2s = r2_score(raw_y_train, raw_y_pred_train_svr_linear)\n# raw_val_svr_linear_r2s = r2_score(raw_y_val, raw_y_pred_val_svr_linear)\n# print(\"Root Mean Squared Error :\", raw_svr_linear_rmse)\n# print(\"R-squared Score (Train) :\", raw_train_svr_linear_r2s)\n# print(\"R-squared Score (Test) :\", raw_val_svr_linear_r2s)\n# print(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:06.765293Z","iopub.execute_input":"2023-12-17T18:41:06.765774Z","iopub.status.idle":"2023-12-17T18:41:09.846375Z","shell.execute_reply.started":"2023-12-17T18:41:06.765728Z","shell.execute_reply":"2023-12-17T18:41:09.844233Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function to find the best value of K based on mean squared error\n\ndef find_k(x_train, y_train, x_test, y_test):\n    error_rate = []    # Finding the error rate for 50 iterations\n\n    for i in range(1, 50):\n        knn = KNeighborsRegressor(n_neighbors = i)    # Building the model with i neighbors\n        knn.fit(x_train, y_train)\n        y_pred = knn.predict(x_test)\n        error_rate.append(mean_squared_error(y_test, y_pred))\n    \n# Ploting the error values to find the best value of k\n    \n    plt.figure(figsize = (8, 8))\n    plt.plot(range(1,50), error_rate, color = 'red', linestyle = 'dashed', marker = 'o', markersize = 10, markerfacecolor = 'blue')\n    plt.title(\"Mean Squared Error vs K-Value\")\n    plt.xlabel(\"K-Value\")\n    plt.ylabel(\"Mean Squared Error\")\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:09.847944Z","iopub.execute_input":"2023-12-17T18:41:09.848352Z","iopub.status.idle":"2023-12-17T18:41:09.856717Z","shell.execute_reply.started":"2023-12-17T18:41:09.848318Z","shell.execute_reply":"2023-12-17T18:41:09.855682Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_k(raw_x_train, raw_y_train, raw_x_val, raw_y_val)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:09.85793Z","iopub.execute_input":"2023-12-17T18:41:09.858908Z","iopub.status.idle":"2023-12-17T18:41:11.930344Z","shell.execute_reply.started":"2023-12-17T18:41:09.85885Z","shell.execute_reply":"2023-12-17T18:41:11.928971Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We can see that the error rate gets lower after 11\n- So deciding to build the model with k = 11","metadata":{}},{"cell_type":"code","source":"# Building KNN regressor with k = 11\nknn_raw = KNeighborsRegressor(n_neighbors = 11)\nknn_raw.fit(raw_x_train, raw_y_train)\nraw_y_pred_train_knn = knn_raw.predict(raw_x_train)\nraw_y_pred_val_knn = knn_raw.predict(raw_x_val)\n\nprint(\"Accuracy Scores for KNN Regressor model on raw data\")\nraw_knn_rmse = mean_squared_error(raw_y_val, raw_y_pred_val_knn)\nraw_train_knn_r2s = r2_score(raw_y_train, raw_y_pred_train_knn)\nraw_val_knn_r2s = r2_score(raw_y_val, raw_y_pred_val_knn)\nprint(\"Mean Squared Error :\", raw_knn_rmse)\nprint(\"R-squared Score (Train) :\", raw_train_knn_r2s)\nprint(\"R-squared Score (Test) :\", raw_val_knn_r2s)\nprint(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:11.931941Z","iopub.execute_input":"2023-12-17T18:41:11.932282Z","iopub.status.idle":"2023-12-17T18:41:12.019995Z","shell.execute_reply.started":"2023-12-17T18:41:11.932251Z","shell.execute_reply":"2023-12-17T18:41:12.018652Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Function for checking the best model out of the tested models for correlation treated dataset\n\ndef check_scores(mse_list, r2_train_list, r2_test_list, data):\n    models_list = ['Linear Regression', 'Lasso Regression', 'Ridge Regression', 'Elastic Net Regression', \"Decission Tree Regression\",\n                   'Random Forest Regression', 'Gradient Boosting Regression', 'LGBoost Regression', 'XGBoost Regression', \n                   'K Nearest Neighbour', 'SVR - Linear', 'SVR - Polynomial', 'SVR - Sigmoid', 'SVR - RBF']\n\n    accuracy = pd.DataFrame({'Method' : models_list, 'Mean Squared Error' : mse_list, 'R2Score(Training)' : r2_train_list,\n                           'R2Score(Testing)' : r2_test_list})\n\n    sns.lineplot(x = 'Method', y = 'Mean Squared Error', data = accuracy, label = 'MSE', color = 'green', linestyle = 'dashed', marker = 'o', markersize = 5, markerfacecolor = 'green')\n    sns.lineplot(x = 'Method', y = 'R2Score(Training)', data = accuracy, label = 'R2Train', color = 'blue', linestyle = 'dashed', marker = 'o', markersize = 5, markerfacecolor = 'blue')\n    sns.lineplot(x = 'Method', y = 'R2Score(Testing)', data = accuracy, label = 'R2Test', color = 'brown', linestyle = 'dashed', marker = 'o', markersize = 5, markerfacecolor = 'brown')\n    plt.xticks(rotation = 90)\n    plt.title(\"Accuracy on %s dataset\" % data)\n    plt.ylim(0.0, 1.0)\n    plt.axhline(y = 0.7, linewidth = 1, color = 'red')\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:12.021487Z","iopub.execute_input":"2023-12-17T18:41:12.021851Z","iopub.status.idle":"2023-12-17T18:41:12.034444Z","shell.execute_reply.started":"2023-12-17T18:41:12.021818Z","shell.execute_reply":"2023-12-17T18:41:12.033033Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the accuracy of models on raw data\n\nraw_mse_list = [raw_lr_rmse, raw_lar_rmse, raw_ridge_rmse, raw_enet_rmse, raw_dtree_rmse, raw_rf_rmse, raw_gb_rmse, raw_lgb_rmse,\n                raw_xgb_rmse, raw_knn_rmse, 0, 0, 0, 0]\n\nraw_r2_train_list = [raw_train_lr_r2s, raw_train_lar_r2s, raw_train_ridge_r2s, raw_train_enet_r2s, raw_train_dtree_r2s,\n                    raw_train_rf_r2s, raw_train_gb_r2s, raw_train_lgb_r2s, raw_train_xgb_r2s, raw_train_knn_r2s, 0, 0, 0, 0]\n\nraw_r2_test_list = [raw_val_lr_r2s, raw_val_lar_r2s, raw_val_ridge_r2s, raw_val_enet_r2s, raw_val_dtree_r2s, raw_val_rf_r2s,\n                   raw_val_gb_r2s, raw_val_lgb_r2s, raw_val_xgb_r2s, raw_val_knn_r2s, 0, 0, 0, 0]\n\ncheck_scores(raw_mse_list, raw_r2_train_list, raw_r2_test_list, 'raw')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:12.036155Z","iopub.execute_input":"2023-12-17T18:41:12.036695Z","iopub.status.idle":"2023-12-17T18:41:12.708401Z","shell.execute_reply.started":"2023-12-17T18:41:12.036656Z","shell.execute_reply":"2023-12-17T18:41:12.707314Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Ensemble models like Random Forest, Gradient Boost Regressor, LightGBM, and XGBoost show good performance with relatively low MSE and high R-squared scores on the test set.\n- Decision Tree performs well but may have a slightly higher MSE compared to ensemble methods.\n- Linear Regression and Ridge Regression show similar performance, indicating that L2 regularization might be more suitable for the data.\n- Lasso Regression and Elastic Net Regression seem to have slightly higher MSE and lower R-squared scores compared to other models.\n- KNN Regressor appears to perform less well on the dataset, with a higher MSE and lower R-squared scores.\n- Checking the above we can see that Random Forest, Gradient Boosting, LGBoost and XGBoost have all performed better\n- They all have men squared erros less than 0.5 \n- All four hae r2 score of more than 0.75 for both train and test","metadata":{}},{"cell_type":"markdown","source":"#### Handling outliers to see if accuracy can be improved","metadata":{}},{"cell_type":"code","source":"# Checking for outliers\n\nolt_x_train = raw_x_train\nolt_y_train = train_df['Rating'] \n\nolt_x_val = raw_x_val\nolt_y_val = validation_df['Rating'] \n\nolt_x_train.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:12.709649Z","iopub.execute_input":"2023-12-17T18:41:12.710026Z","iopub.status.idle":"2023-12-17T18:41:12.727948Z","shell.execute_reply.started":"2023-12-17T18:41:12.709994Z","shell.execute_reply":"2023-12-17T18:41:12.726711Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"olt_x_val.head(2)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:12.729866Z","iopub.execute_input":"2023-12-17T18:41:12.730337Z","iopub.status.idle":"2023-12-17T18:41:12.74711Z","shell.execute_reply.started":"2023-12-17T18:41:12.7303Z","shell.execute_reply":"2023-12-17T18:41:12.745854Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for outliers using boxplots\n\nplt.figure(figsize=(12, 12))\n\nfor col in olt_x_train.select_dtypes(include=['number']).columns:\n    plt.subplot(3, 3, olt_x_train.columns.get_loc(col) + 1)  # Adjust the subplot layout as needed\n    sns.boxplot(x=olt_x_train[col])\n    plt.title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:12.748972Z","iopub.execute_input":"2023-12-17T18:41:12.749393Z","iopub.status.idle":"2023-12-17T18:41:15.127584Z","shell.execute_reply.started":"2023-12-17T18:41:12.749344Z","shell.execute_reply":"2023-12-17T18:41:15.126261Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for outliers using boxen plot\n\nplt.figure(figsize=(12, 12))\n\nfor col in olt_x_train.select_dtypes(include=['number']).columns:\n    plt.subplot(3, 3, olt_x_train.columns.get_loc(col) + 1)  # Adjust the subplot layout as needed\n    sns.boxenplot(x=olt_x_train[col])\n    plt.title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:15.129062Z","iopub.execute_input":"2023-12-17T18:41:15.129426Z","iopub.status.idle":"2023-12-17T18:41:17.521279Z","shell.execute_reply.started":"2023-12-17T18:41:15.129393Z","shell.execute_reply":"2023-12-17T18:41:17.520056Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for outliers using dist plot\n\nplt.figure(figsize=(12, 12))\n\nfor col in olt_x_train.select_dtypes(include=['number']).columns:\n    plt.subplot(3, 3, olt_x_train.columns.get_loc(col) + 1)  # Adjust the subplot layout as needed\n    sns.distplot(x=olt_x_train[col])\n    plt.title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:17.523106Z","iopub.execute_input":"2023-12-17T18:41:17.523697Z","iopub.status.idle":"2023-12-17T18:41:22.024643Z","shell.execute_reply.started":"2023-12-17T18:41:17.523646Z","shell.execute_reply":"2023-12-17T18:41:22.023195Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- We see outliers and skewness throughout the dataset\n    - Using IQR method on all columns except Year\n- Building the prediction models after treating the outliers.","metadata":{}},{"cell_type":"code","source":"# Outlier treatment\n\nout_list = ['Duration', 'Votes', 'Director_TargetEncoded', 'Actor 1_TargetEncoded', 'Actor 2_TargetEncoded',\n            'Actor 3_TargetEncoded', 'Genre_TargetEncoded']\n\nfor i in olt_x_train.columns:\n    col_q1 = olt_x_train[i].quantile(0.25)\n    col_q3 = olt_x_train[i].quantile(0.75)\n    col_iqr = col_q3 - col_q1\n    \n    upper_limit = col_q3 + (1.5 * col_iqr)\n    lower_limit = col_q1 - (1.5 * col_iqr)\n    \n    if i in out_list:\n        olt_x_train[i] = np.where(olt_x_train[i] > upper_limit, upper_limit, \n                                   np.where(olt_x_train[i] < lower_limit, lower_limit, olt_x_train[i]))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:22.026235Z","iopub.execute_input":"2023-12-17T18:41:22.026696Z","iopub.status.idle":"2023-12-17T18:41:22.057285Z","shell.execute_reply.started":"2023-12-17T18:41:22.026648Z","shell.execute_reply":"2023-12-17T18:41:22.055858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the visuals to see if the outliers are fixed\n\nplt.figure(figsize=(12, 12))\n\nfor col in olt_x_train.select_dtypes(include=['number']).columns:\n    plt.subplot(3, 3, olt_x_train.columns.get_loc(col) + 1)  # Adjust the subplot layout as needed\n    sns.boxplot(x=olt_x_train[col])\n    plt.title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:22.059161Z","iopub.execute_input":"2023-12-17T18:41:22.059654Z","iopub.status.idle":"2023-12-17T18:41:24.449844Z","shell.execute_reply.started":"2023-12-17T18:41:22.059597Z","shell.execute_reply":"2023-12-17T18:41:24.448298Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for outliers using boxen plot\n\nplt.figure(figsize=(12, 12))\n\nfor col in olt_x_train.select_dtypes(include=['number']).columns:\n    plt.subplot(3, 3, olt_x_train.columns.get_loc(col) + 1)  # Adjust the subplot layout as needed\n    sns.boxenplot(x=olt_x_train[col])\n    plt.title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:24.452169Z","iopub.execute_input":"2023-12-17T18:41:24.45268Z","iopub.status.idle":"2023-12-17T18:41:26.995602Z","shell.execute_reply.started":"2023-12-17T18:41:24.452631Z","shell.execute_reply":"2023-12-17T18:41:26.994435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking for outliers using dist plot\n\nplt.figure(figsize=(12, 12))\n\nfor col in olt_x_train.select_dtypes(include=['number']).columns:\n    plt.subplot(3, 3, olt_x_train.columns.get_loc(col) + 1)  # Adjust the subplot layout as needed\n    sns.distplot(x=olt_x_train[col])\n    plt.title(col)\n\nplt.tight_layout()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:26.997411Z","iopub.execute_input":"2023-12-17T18:41:26.998205Z","iopub.status.idle":"2023-12-17T18:41:31.385221Z","shell.execute_reply.started":"2023-12-17T18:41:26.998155Z","shell.execute_reply":"2023-12-17T18:41:31.384008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using the same treatment on the validation dataset\n\nfor i in olt_x_val.columns:\n    col_q1 = olt_x_val[i].quantile(0.25)\n    col_q3 = olt_x_val[i].quantile(0.75)\n    col_iqr = col_q3 - col_q1\n    \n    upper_limit = col_q3 + (1.5 * col_iqr)\n    lower_limit = col_q1 - (1.5 * col_iqr)\n    \n    if i in out_list:\n        olt_x_val[i] = np.where(olt_x_val[i] > upper_limit, upper_limit, \n                                   np.where(olt_x_val[i] < lower_limit, lower_limit, olt_x_val[i]))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:31.386817Z","iopub.execute_input":"2023-12-17T18:41:31.387791Z","iopub.status.idle":"2023-12-17T18:41:31.416784Z","shell.execute_reply.started":"2023-12-17T18:41:31.387749Z","shell.execute_reply":"2023-12-17T18:41:31.415551Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"#### Building the models on outlier treated data","metadata":{}},{"cell_type":"code","source":"# Building the models\n\n# Linear Regression model\nlinear_model_olt = LinearRegression()\nlinear_model_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_lr = linear_model_olt.predict(olt_x_train)\nolt_y_pred_val_lr = linear_model_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Linear Regression model on outlier treated data\")\nolt_lr_mse = mean_squared_error(olt_y_val, olt_y_pred_val_lr)\nolt_train_lr_r2s = r2_score(olt_y_train, olt_y_pred_train_lr)\nolt_val_lr_r2s = r2_score(olt_y_val, olt_y_pred_val_lr)\nprint(\"Mean Squared Error :\", olt_lr_mse)\nprint(\"R-squared Score (Train) :\", olt_train_lr_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_lr_r2s)\nprint(\"**************\" * 7)\n\n# Lasso Regression (L1 Regularization) going with alpha = 0.05, after checking various values\nlasso_model_olt = Lasso(alpha = 0.05)\nlasso_model_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_lar = lasso_model_olt.predict(olt_x_train)\nolt_y_pred_val_lar = lasso_model_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Lasso Regression (L1 Regularization) model on outlier treated data\")\nolt_lar_mse = mean_squared_error(olt_y_val, olt_y_pred_val_lar)\nolt_train_lar_r2s = r2_score(olt_y_train, olt_y_pred_train_lar)\nolt_val_lar_r2s = r2_score(olt_y_val, olt_y_pred_val_lar)\nprint(\"Mean Squared Error :\", olt_lar_mse)\nprint(\"R-squared Score (Train) :\", olt_train_lar_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_lar_r2s)\nprint(\"**************\" * 7)\n\n# Ridge Regression (L2 Regularization) going with alpha = 0.05, after checking various values\nridge_model_olt = Ridge(alpha = 0.05)\nridge_model_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_ridge = ridge_model_olt.predict(olt_x_train)\nolt_y_pred_val_ridge = ridge_model_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Ridge Regression (L2 Regularization) model on outlier treated data\")\nolt_ridge_mse = mean_squared_error(olt_y_val, olt_y_pred_val_ridge)\nolt_train_ridge_r2s = r2_score(olt_y_train, olt_y_pred_train_ridge)\nolt_val_ridge_r2s = r2_score(olt_y_val, olt_y_pred_val_ridge)\nprint(\"Mean Squared Error :\", olt_ridge_mse)\nprint(\"R-squared Score (Train) :\", olt_train_ridge_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_ridge_r2s)\nprint(\"**************\" * 7)\n\n# Elastic Net Regression (L1 and L2 Regularizations) going with alpha = 0.05, after checking various values\nenet_model_olt = ElasticNet(alpha = 0.05, random_state = 101)\nenet_model_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_enet = enet_model_olt.predict(olt_x_train)\nolt_y_pred_val_enet = enet_model_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Elastic Net Regression (L1 and L2 Regularizations) model on outlier treated data\")\nolt_enet_mse = mean_squared_error(olt_y_val, olt_y_pred_val_enet)\nolt_train_enet_r2s = r2_score(olt_y_train, olt_y_pred_train_enet)\nolt_val_enet_r2s = r2_score(olt_y_val, olt_y_pred_val_enet)\nprint(\"Mean Squared Error :\", olt_enet_mse)\nprint(\"R-squared Score (Train) :\", olt_train_enet_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_enet_r2s)\nprint(\"**************\" * 7)\n\n# Decission Tree regression - Choosing max depth as 6 after trying different values\ndtree_olt = DecisionTreeRegressor(max_depth = 6)\ndtree_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_dtree = dtree_olt.predict(olt_x_train)\nolt_y_pred_val_dtree = dtree_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Decision Tree model on outlier treated data\")\nolt_dtree_mse = mean_squared_error(olt_y_val, olt_y_pred_val_dtree)\nolt_train_dtree_r2s = r2_score(olt_y_train, olt_y_pred_train_dtree)\nolt_val_dtree_r2s = r2_score(olt_y_val, olt_y_pred_val_dtree)\nprint(\"Mean Squared Error :\", olt_dtree_mse)\nprint(\"R-squared Score (Train) :\", olt_train_dtree_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_dtree_r2s)\nprint(\"**************\" * 7)\n\n# Random Forest regression - Choosing max depth as 6 after trying different values\n# Choosing the parameters after trying different values\nrf_olt = RandomForestRegressor(n_estimators = 100, random_state = 1, max_depth = 6)\nrf_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_rf = rf_olt.predict(olt_x_train)\nolt_y_pred_val_rf = rf_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Random Forest model on outlier treated data\")\nolt_rf_mse = mean_squared_error(olt_y_val, olt_y_pred_val_rf)\nolt_train_rf_r2s = r2_score(olt_y_train, olt_y_pred_train_rf)\nolt_val_rf_r2s = r2_score(olt_y_val, olt_y_pred_val_rf)\nprint(\"Mean Squared Error :\", olt_rf_mse)\nprint(\"R-squared Score (Train) :\", olt_train_rf_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_rf_r2s)\nprint(\"**************\" * 7)\n\n# Gradient Boosting Regression model\ngb_olt = GradientBoostingRegressor(n_estimators = 100, max_depth = 3, random_state = 123)\ngb_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_gb = gb_olt.predict(olt_x_train)\nolt_y_pred_val_gb = gb_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for Gradient Boost Regressor model on outlier treated data\")\nolt_gb_mse = mean_squared_error(olt_y_val, olt_y_pred_val_gb)\nolt_train_gb_r2s = r2_score(olt_y_train, olt_y_pred_train_gb)\nolt_val_gb_r2s = r2_score(olt_y_val, olt_y_pred_val_gb)\nprint(\"Mean Squared Error :\", olt_gb_mse)\nprint(\"R-squared Score (Train) :\", olt_train_gb_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_gb_r2s)\nprint(\"**************\" * 7)\n\n# LGBoost Regression model - Max depth and Num_leaves = 3 after testing various values\nlgb_olt = lgb.LGBMRegressor(random_state = 11, max_depth = 3, num_leaves = 3, force_col_wise = True)\nlgb_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_lgb = lgb_olt.predict(olt_x_train)\nolt_y_pred_val_lgb = lgb_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for LGBoost model on outlier treated data\")\nolt_lgb_mse = mean_squared_error(olt_y_val, olt_y_pred_val_lgb)\nolt_train_lgb_r2s = r2_score(olt_y_train, olt_y_pred_train_lgb)\nolt_val_lgb_r2s = r2_score(olt_y_val, olt_y_pred_val_lgb)\nprint(\"Mean Squared Error :\", olt_lgb_mse)\nprint(\"R-squared Score (Train) :\", olt_train_lgb_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_lgb_r2s)\nprint(\"**************\" * 7)\n\n# XGBoost Regression model - Max depth = 2 after testing various values\nxgb_olt = xgb.XGBRegressor(random_state = 111, max_depth = 2)\nxgb_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_xgb = xgb_olt.predict(olt_x_train)\nolt_y_pred_val_xgb = xgb_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for XGBoost model on outlier treated data\")\nolt_xgb_mse = mean_squared_error(olt_y_val, olt_y_pred_val_xgb)\nolt_train_xgb_r2s = r2_score(olt_y_train, olt_y_pred_train_xgb)\nolt_val_xgb_r2s = r2_score(olt_y_val, olt_y_pred_val_xgb)\nprint(\"Mean Squared Error :\", olt_xgb_mse)\nprint(\"R-squared Score (Train) :\", olt_train_xgb_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_xgb_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Linear kernel\n# It is computationaly very expensive and gives very poor results also, as below\n\n# svr_linear_olt = SVR(kernel = 'linear')\n# svr_linear_olt.fit(olt_x_train, olt_y_train)\n# olt_y_pred_train_svr_linear = svr_linear_olt.predict(olt_x_train)\n# olt_y_pred_val_svr_linear = svr_linear_olt.predict(olt_x_val)\n# Accuracy Scores for Support Vector Model with linear kernel on outlier treated data\n# Root Mean Squared Error : 1.3067999090336686\n# R-squared Score (Train) : 0.31752905572919654\n# R-squared Score (Test) : 0.306558443102405\n# print(\"Accuracy Scores for Support Vector Model with linear kernel on outlier treated data\")\n# olt_svr_linear_mse = mean_squared_error(olt_y_val, olt_y_pred_val_svr_linear)\n# olt_train_svr_linear_r2s = r2_score(olt_y_train, olt_y_pred_train_svr_linear)\n# olt_val_svr_linear_r2s = r2_score(olt_y_val, olt_y_pred_val_svr_linear)\n# print(\"Root Mean Squared Error :\", olt_svr_linear_mse)\n# print(\"R-squared Score (Train) :\", olt_train_svr_linear_r2s)\n# print(\"R-squared Score (Test) :\", olt_val_svr_linear_r2s)\n# print(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:31.421806Z","iopub.execute_input":"2023-12-17T18:41:31.422258Z","iopub.status.idle":"2023-12-17T18:41:34.218832Z","shell.execute_reply.started":"2023-12-17T18:41:31.422219Z","shell.execute_reply":"2023-12-17T18:41:34.217928Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_k(olt_x_train, olt_y_train, olt_x_val, olt_y_val)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:34.220323Z","iopub.execute_input":"2023-12-17T18:41:34.22092Z","iopub.status.idle":"2023-12-17T18:41:36.440739Z","shell.execute_reply.started":"2023-12-17T18:41:34.220861Z","shell.execute_reply":"2023-12-17T18:41:36.439769Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Choosing k = 13 as the error is lowest at that point","metadata":{}},{"cell_type":"code","source":"# Building KNN regressor with k = 13\n\nknn_olt = KNeighborsRegressor(n_neighbors = 13)\nknn_olt.fit(olt_x_train, olt_y_train)\nolt_y_pred_train_knn = knn_olt.predict(olt_x_train)\nolt_y_pred_val_knn = knn_olt.predict(olt_x_val)\n\nprint(\"Accuracy Scores for KNN Regressor model on outlier treated data\")\nolt_knn_mse = mean_squared_error(olt_y_val, olt_y_pred_val_knn)\nolt_train_knn_r2s = r2_score(olt_y_train, olt_y_pred_train_knn)\nolt_val_knn_r2s = r2_score(olt_y_val, olt_y_pred_val_knn)\nprint(\"Mean Squared Error :\", olt_knn_mse)\nprint(\"R-squared Score (Train) :\", olt_train_knn_r2s)\nprint(\"R-squared Score (Test) :\", olt_val_knn_r2s)\nprint(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:36.442323Z","iopub.execute_input":"2023-12-17T18:41:36.443041Z","iopub.status.idle":"2023-12-17T18:41:36.545354Z","shell.execute_reply.started":"2023-12-17T18:41:36.442982Z","shell.execute_reply":"2023-12-17T18:41:36.544143Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the accuracy of models on raw data\n\nolt_mse_list = [olt_lr_mse, olt_lar_mse, olt_ridge_mse, olt_enet_mse, olt_dtree_mse, olt_rf_mse, olt_gb_mse, olt_lgb_mse,\n                olt_xgb_mse, olt_knn_mse, 0, 0, 0, 0]\n\nolt_r2_train_list = [olt_train_lr_r2s, olt_train_lar_r2s, olt_train_ridge_r2s, olt_train_enet_r2s, olt_train_dtree_r2s,\n                    olt_train_rf_r2s, olt_train_gb_r2s, olt_train_lgb_r2s, olt_train_xgb_r2s, olt_train_knn_r2s, 0, 0, 0, 0]\n\nolt_r2_test_list = [olt_val_lr_r2s, olt_val_lar_r2s, olt_val_ridge_r2s, olt_val_enet_r2s, olt_val_dtree_r2s, olt_val_rf_r2s,\n                   olt_val_gb_r2s, olt_val_lgb_r2s, olt_val_xgb_r2s, olt_val_knn_r2s, 0, 0, 0, 0]\n\ncheck_scores(olt_mse_list, olt_r2_train_list, olt_r2_test_list, 'outlier treated')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:36.547155Z","iopub.execute_input":"2023-12-17T18:41:36.547914Z","iopub.status.idle":"2023-12-17T18:41:37.213284Z","shell.execute_reply.started":"2023-12-17T18:41:36.547841Z","shell.execute_reply":"2023-12-17T18:41:37.212132Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Outlier treatment has led to changes in performance metrics for some models compared to the evaluation on raw data.\n- Ensembles like Random Forest and Gradient Boost Regressor still perform well after outlier treatment, with relatively low MSE and high R-squared scores.\n- Support Vector Model with a Linear Kernel and KNN Regressor show a decrease in performance on the test set after outlier treatment.\n- Linear models (Linear Regression, Lasso, Ridge, Elastic Net) also show consistent or improved performance after outlier treatment.\n- Checking the above we can see that Random Forest, Gradient Boosting, LGBoost and XGBoost have all performed better\n- They all have men squared erros less than 0.5 \n- All four hae r2 score of more than 0.75 for both train and test\n- Additionally we have some more models reaching 0.7 accuracy\n- Support Vector Regression techniques are computationaly very high and low performance, even after outlier treatment","metadata":{}},{"cell_type":"markdown","source":"#### Using the Standared scaler method on the raw data","metadata":{}},{"cell_type":"code","source":"ssc = StandardScaler()\n\nscld_raw_x_train = pd.DataFrame(ssc.fit_transform(raw_x_train))\nscld_raw_y_train = train_df['Rating']\n\nscld_raw_x_val = pd.DataFrame(ssc.fit_transform(raw_x_val))\nscld_raw_y_val = validation_df['Rating']","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:37.215074Z","iopub.execute_input":"2023-12-17T18:41:37.215824Z","iopub.status.idle":"2023-12-17T18:41:37.23474Z","shell.execute_reply.started":"2023-12-17T18:41:37.215763Z","shell.execute_reply":"2023-12-17T18:41:37.233348Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the models on scalled data and checking the accuracy\n\n# Linear Regression model\nlinear_model_raw_scld = LinearRegression()\nlinear_model_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_lr = linear_model_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_lr = linear_model_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Linear Regression model on scaled raw data\")\nscld_raw_lr_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_lr)\nscld_raw_train_lr_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_lr)\nscld_raw_val_lr_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_lr)\nprint(\"Mean Squared Error :\", scld_raw_lr_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_lr_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_lr_r2s)\nprint(\"**************\" * 7)\n\n# Lasso Regression (L1 Regularization) going with alpha = 0.05, after checking various values\nlasso_model_raw_scld = Lasso(alpha = 0.05)\nlasso_model_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_lar = lasso_model_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_lar = lasso_model_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Lasso Regression (L1 Regularization) model on sclaed raw data\")\nscld_raw_lar_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_lar)\nscld_raw_train_lar_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_lar)\nscld_raw_val_lar_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_lar)\nprint(\"Mean Squared Error :\", scld_raw_lar_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_lar_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_lar_r2s)\nprint(\"**************\" * 7)\n\n# Ridge Regression (L2 Regularization) going with alpha = 0.05, after checking various values\nridge_model_raw_scld = Ridge(alpha = 0.05)\nridge_model_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_ridge = ridge_model_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_ridge = ridge_model_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Ridge Regression (L2 Regularization) model on scaled raw data\")\nscld_raw_ridge_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_ridge)\nscld_raw_train_ridge_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_ridge)\nscld_raw_val_ridge_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_ridge)\nprint(\"Mean Squared Error :\", scld_raw_ridge_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_ridge_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_ridge_r2s)\nprint(\"**************\" * 7)\n\n# Elastic Net Regression (L1 and L2 Regularizations) going with alpha = 0.05, after checking various values\nenet_model_raw_scld = ElasticNet(alpha = 0.05, random_state = 101)\nenet_model_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_enet = enet_model_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_enet = enet_model_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Elastic Net Regression (L1 and L2 Regularizations) model on scalled raw data\")\nscld_raw_enet_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_enet)\nscld_raw_train_enet_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_enet)\nscld_raw_val_enet_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_enet)\nprint(\"Mean Squared Error :\", scld_raw_enet_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_enet_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_enet_r2s)\nprint(\"**************\" * 7)\n\n# Decission Tree regression - Choosing max depth as 6 after trying different values\ndtree_raw_scld = DecisionTreeRegressor(max_depth = 7)\ndtree_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_dtree = dtree_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_dtree = dtree_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Decision Tree model on scaled raw data\")\nscld_raw_dtree_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_dtree)\nscld_raw_train_dtree_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_dtree)\nscld_raw_val_dtree_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_dtree)\nprint(\"Mean Squared Error :\", scld_raw_dtree_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_dtree_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_dtree_r2s)\nprint(\"**************\" * 7)\n\n# Random Forest regression - Choosing max depth as 6 after trying different values\n# Choosing the parameters after trying different values\nrf_raw_scld = RandomForestRegressor(n_estimators = 100, random_state = 1, max_depth = 8)\nrf_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_rf = rf_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_rf = rf_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Random Forest model on scalled raw data\")\nscld_raw_rf_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_rf)\nscld_raw_train_rf_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_rf)\nscld_raw_val_rf_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_rf)\nprint(\"Mean Squared Error :\", scld_raw_rf_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_rf_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_rf_r2s)\nprint(\"**************\" * 7)\n\n# Gradient Boosting Regression model\ngb_raw_scld = GradientBoostingRegressor(n_estimators = 100, max_depth = 3, random_state = 123)\ngb_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_gb = gb_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_gb = gb_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Gradient Boost Regressor model on scaled raw data\")\nscld_raw_gb_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_gb)\nscld_raw_train_gb_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_gb)\nscld_raw_val_gb_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_gb)\nprint(\"Mean Squared Error :\", scld_raw_gb_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_gb_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_gb_r2s)\nprint(\"**************\" * 7)\n\n# LGBoost Regression model - Max depth = 6 and Num_leaves = 3 after testing various values\nlgb_raw_scld = lgb.LGBMRegressor(random_state = 11, max_depth = 6, num_leaves = 3, force_col_wise = True)\nlgb_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_lgb = lgb_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_lgb = lgb_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for LGBoost model on scaled raw data\")\nscld_raw_lgb_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_lgb)\nscld_raw_train_lgb_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_lgb)\nscld_raw_val_lgb_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_lgb)\nprint(\"Mean Squared Error :\", scld_raw_lgb_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_lgb_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_lgb_r2s)\nprint(\"**************\" * 7)\n\n# XGBoost Regression model - Max depth = 2 after testing various values\nxgb_raw_scld = xgb.XGBRegressor(random_state = 111, max_depth = 2)\nxgb_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_xgb = xgb_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_xgb = xgb_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for XGBoost model on scaled raw data\")\nscld_raw_xgb_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_xgb)\nscld_raw_train_xgb_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_xgb)\nscld_raw_val_xgb_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_xgb)\nprint(\"Mean Squared Error :\", scld_raw_xgb_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_xgb_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_xgb_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Linear kernel\nsvr_linear_raw_scld = SVR(kernel = 'linear')\nsvr_linear_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_svr_linear = svr_linear_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_svr_linear = svr_linear_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with linear kernel on sclaed data\")\nscld_raw_svr_linear_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_svr_linear)\nscld_raw_train_svr_linear_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_svr_linear)\nscld_raw_val_svr_linear_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_svr_linear)\nprint(\"Root Mean Squared Error :\", scld_raw_svr_linear_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_svr_linear_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_svr_linear_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Polynomial kernel\nsvr_poly_raw_scld = SVR(kernel = 'poly')\nsvr_poly_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_svr_poly = svr_poly_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_svr_poly = svr_poly_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with polynomial kernel on sclaed data\")\nscld_raw_svr_poly_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_svr_poly)\nscld_raw_train_svr_poly_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_svr_poly)\nscld_raw_val_svr_poly_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_svr_poly)\nprint(\"Root Mean Squared Error :\", scld_raw_svr_poly_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_svr_poly_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_svr_poly_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Sigmoid kernel\nsvr_sigmoid_raw_scld = SVR(kernel = 'sigmoid')\nsvr_sigmoid_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_svr_sigmoid = svr_sigmoid_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_svr_sigmoid = svr_sigmoid_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with sigmoid kernel on sclaed data\")\nscld_raw_svr_sigmoid_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_svr_sigmoid)\nscld_raw_train_svr_sigmoid_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_svr_sigmoid)\nscld_raw_val_svr_sigmoid_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_svr_sigmoid)\nprint(\"Root Mean Squared Error :\", scld_raw_svr_sigmoid_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_svr_sigmoid_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_svr_sigmoid_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - RBF kernel\nsvr_rbf_raw_scld = SVR(kernel = 'rbf')\nsvr_rbf_raw_scld.fit(scld_raw_x_train, scld_raw_y_train)\nscld_raw_y_pred_train_svr_rbf = svr_rbf_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_svr_rbf = svr_rbf_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with rbf kernel on sclaed data\")\nscld_raw_svr_rbf_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_svr_rbf)\nscld_raw_train_svr_rbf_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_svr_rbf)\nscld_raw_val_svr_rbf_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_svr_rbf)\nprint(\"Root Mean Squared Error :\", scld_raw_svr_rbf_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_svr_rbf_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_svr_rbf_r2s)\nprint(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:37.237347Z","iopub.execute_input":"2023-12-17T18:41:37.238086Z","iopub.status.idle":"2023-12-17T18:41:58.682901Z","shell.execute_reply.started":"2023-12-17T18:41:37.23804Z","shell.execute_reply":"2023-12-17T18:41:58.681532Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_k(scld_raw_x_train, scld_raw_y_train, scld_raw_x_val, scld_raw_y_val)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:41:58.684798Z","iopub.execute_input":"2023-12-17T18:41:58.685218Z","iopub.status.idle":"2023-12-17T18:42:04.854819Z","shell.execute_reply.started":"2023-12-17T18:41:58.685182Z","shell.execute_reply":"2023-12-17T18:42:04.853431Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Lowest error rate occurs first at k = 15\n- So proceeding with k = 15 to build the model","metadata":{}},{"cell_type":"code","source":"# Building KNN regressor with k = 11\nknn_raw_scld = KNeighborsRegressor(n_neighbors = 11)\nknn_raw_scld.fit(raw_x_train, raw_y_train)\nscld_raw_y_pred_train_knn = knn_raw_scld.predict(scld_raw_x_train)\nscld_raw_y_pred_val_knn = knn_raw_scld.predict(scld_raw_x_val)\n\nprint(\"Accuracy Scores for KNN Regressor model on scaled raw data\")\nscld_raw_knn_mse = mean_squared_error(scld_raw_y_val, scld_raw_y_pred_val_knn)\nscld_raw_train_knn_r2s = r2_score(scld_raw_y_train, scld_raw_y_pred_train_knn)\nscld_raw_val_knn_r2s = r2_score(scld_raw_y_val, scld_raw_y_pred_val_knn)\nprint(\"Mean Squared Error :\", scld_raw_knn_mse)\nprint(\"R-squared Score (Train) :\", scld_raw_train_knn_r2s)\nprint(\"R-squared Score (Test) :\", scld_raw_val_knn_r2s)\nprint(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:04.856652Z","iopub.execute_input":"2023-12-17T18:42:04.85779Z","iopub.status.idle":"2023-12-17T18:42:05.021402Z","shell.execute_reply.started":"2023-12-17T18:42:04.85774Z","shell.execute_reply":"2023-12-17T18:42:05.020169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the accuracy of models on raw data\n\nscld_raw_mse_list = [scld_raw_lr_mse, scld_raw_lar_mse, scld_raw_ridge_mse, scld_raw_enet_mse, scld_raw_dtree_mse, \n                     scld_raw_rf_mse, scld_raw_gb_mse, scld_raw_lgb_mse, scld_raw_xgb_mse, scld_raw_knn_mse, \n                     scld_raw_svr_linear_mse, scld_raw_svr_poly_mse, scld_raw_svr_sigmoid_mse, scld_raw_svr_rbf_mse]\n\nscld_raw_r2_train_list = [scld_raw_train_lr_r2s, scld_raw_train_lar_r2s, scld_raw_train_ridge_r2s, scld_raw_train_enet_r2s,\n                          scld_raw_train_dtree_r2s, scld_raw_train_rf_r2s, scld_raw_train_gb_r2s, scld_raw_train_lgb_r2s, \n                          scld_raw_train_xgb_r2s, scld_raw_train_knn_r2s, scld_raw_train_svr_linear_r2s, \n                          scld_raw_train_svr_poly_r2s, scld_raw_train_svr_sigmoid_r2s, scld_raw_train_svr_rbf_r2s]\n\nscld_raw_r2_test_list = [scld_raw_val_lr_r2s, scld_raw_val_lar_r2s, scld_raw_val_ridge_r2s, scld_raw_val_enet_r2s, \n                         scld_raw_val_dtree_r2s, scld_raw_val_rf_r2s, scld_raw_val_gb_r2s, scld_raw_val_lgb_r2s, \n                         scld_raw_val_xgb_r2s, scld_raw_val_knn_r2s, scld_raw_val_svr_linear_r2s, scld_raw_val_svr_poly_r2s, \n                         scld_raw_val_svr_sigmoid_r2s, scld_raw_val_svr_rbf_r2s]\n\ncheck_scores(scld_raw_mse_list, scld_raw_r2_train_list, scld_raw_r2_test_list, 'scaled raw')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:05.023318Z","iopub.execute_input":"2023-12-17T18:42:05.024116Z","iopub.status.idle":"2023-12-17T18:42:05.711358Z","shell.execute_reply.started":"2023-12-17T18:42:05.024066Z","shell.execute_reply":"2023-12-17T18:42:05.709771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Tree-based models (Random Forest, Gradient Boost Regressor, LightGBM, XGBoost) and linear models (Linear Regression, Lasso, Ridge, Elastic Net) perform well on the test set.\n- Support Vector Models with a linear and RBF kernel show good performance, while those with polynomial and sigmoid kernels exhibit poor performance, possibly indicating overfitting or inappropriate kernel choices.\n-  KNN Regressor shows poor performance with negative R-squared scores, suggesting that it might not be suitable for the given data.\n- Linear Regression, Lasso Regression, and Ridge Regression show similar performance, with decent R-squared scores on the test set.\n- Decision Tree and Random Forest perform reasonably well, with Random Forest outperforming Decision Tree in terms of R-squared scores.\n- Gradient Boost Regressor, LightGBM, and XGBoost show good performance, with R-squared scores around 0.82 on the test set.\n- KNN Regressor seems to perform poorly in comparison to the other models, with negative R-squared scores indicating that it might not be well-suited for the data.","metadata":{}},{"cell_type":"markdown","source":"#### Using Standard Scaler of outlier treated data","metadata":{}},{"cell_type":"code","source":"ssc2 = StandardScaler()\n\nscld_olt_x_train = pd.DataFrame(ssc2.fit_transform(olt_x_train))\nscld_olt_y_train = train_df['Rating']\n\nscld_olt_x_val = pd.DataFrame(ssc.fit_transform(olt_x_val))\nscld_olt_y_val = validation_df['Rating']","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:05.713364Z","iopub.execute_input":"2023-12-17T18:42:05.714213Z","iopub.status.idle":"2023-12-17T18:42:05.732606Z","shell.execute_reply.started":"2023-12-17T18:42:05.714164Z","shell.execute_reply":"2023-12-17T18:42:05.731335Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the models\n\n# Linear Regression model\nlinear_model_olt_scld = LinearRegression()\nlinear_model_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_lr = linear_model_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_lr = linear_model_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Linear Regression model on scaled outlier treated data\")\nscld_olt_lr_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_lr)\nscld_olt_train_lr_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_lr)\nscld_olt_val_lr_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_lr)\nprint(\"Mean Squared Error :\", scld_olt_lr_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_lr_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_lr_r2s)\nprint(\"**************\" * 7)\n\n# Lasso Regression (L1 Regularization) going with alpha = 0.05, after checking various values\nlasso_model_olt_scld = Lasso(alpha = 0.05)\nlasso_model_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_lar = lasso_model_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_lar = lasso_model_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Lasso Regression (L1 Regularization) model on scaled outlier treated data\")\nscld_olt_lar_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_lar)\nscld_olt_train_lar_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_lar)\nscld_olt_val_lar_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_lar)\nprint(\"Mean Squared Error :\", scld_olt_lar_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_lar_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_lar_r2s)\nprint(\"**************\" * 7)\n\n# Ridge Regression (L2 Regularization) going with alpha = 0.05, after checking various values\nridge_model_olt_scld = Ridge(alpha = 0.05)\nridge_model_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_ridge = ridge_model_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_ridge = ridge_model_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Ridge Regression (L2 Regularization) model on scaled outlier treated data\")\nscld_olt_ridge_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_ridge)\nscld_olt_train_ridge_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_ridge)\nscld_olt_val_ridge_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_ridge)\nprint(\"Mean Squared Error :\", scld_olt_ridge_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_ridge_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_ridge_r2s)\nprint(\"**************\" * 7)\n\n# Elastic Net Regression (L1 and L2 Regularizations) going with alpha = 0.05, after checking various values\nenet_model_olt_scld = ElasticNet(alpha = 0.05, random_state = 101)\nenet_model_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_enet = enet_model_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_enet = enet_model_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Elastic Net Regression (L1 and L2 Regularizations) model on scaled outlier treated data\")\nscld_olt_enet_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_enet)\nscld_olt_train_enet_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_enet)\nscld_olt_val_enet_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_enet)\nprint(\"Mean Squared Error :\", scld_olt_enet_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_enet_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_enet_r2s)\nprint(\"**************\" * 7)\n\n# Decission Tree regression - Choosing max depth as 7 after trying different values\ndtree_olt_scld = DecisionTreeRegressor(max_depth = 7)\ndtree_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_dtree = dtree_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_dtree = dtree_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Decision Tree model on scaled outlier treated data\")\nscld_olt_dtree_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_dtree)\nscld_olt_train_dtree_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_dtree)\nscld_olt_val_dtree_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_dtree)\nprint(\"Mean Squared Error :\", scld_olt_dtree_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_dtree_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_dtree_r2s)\nprint(\"**************\" * 7)\n\n# Random Forest regression - Choosing max depth as 6 after trying different values\n# Choosing the parameters after trying different values\nrf_olt_scld = RandomForestRegressor(n_estimators = 100, random_state = 1, max_depth = 8)\nrf_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_rf = rf_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_rf = rf_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Random Forest model on scaled outlier treated data\")\nscld_olt_rf_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_rf)\nscld_olt_train_rf_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_rf)\nscld_olt_val_rf_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_rf)\nprint(\"Mean Squared Error :\", scld_olt_rf_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_rf_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_rf_r2s)\nprint(\"**************\" * 7)\n\n# Gradient Boosting Regression model\ngb_olt_scld = GradientBoostingRegressor(n_estimators = 100, max_depth = 5, random_state = 123)\ngb_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_gb = gb_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_gb = gb_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Gradient Boost Regressor model on scaled outlier treated data\")\nscld_olt_gb_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_gb)\nscld_olt_train_gb_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_gb)\nscld_olt_val_gb_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_gb)\nprint(\"Mean Squared Error :\", scld_olt_gb_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_gb_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_gb_r2s)\nprint(\"**************\" * 7)\n\n# LGBoost Regression model - Max depth and Num_leaves = 3 after testing various values\nlgb_olt_scld = lgb.LGBMRegressor(random_state = 11, max_depth = 3, num_leaves = 3, force_col_wise = True)\nlgb_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_lgb = lgb_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_lgb = lgb_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for LGBoost model on scaled outlier treated data\")\nscld_olt_lgb_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_lgb)\nscld_olt_train_lgb_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_lgb)\nscld_olt_val_lgb_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_lgb)\nprint(\"Mean Squared Error :\", scld_olt_lgb_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_lgb_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_lgb_r2s)\nprint(\"**************\" * 7)\n\n# XGBoost Regression model - Max depth = 2 after testing various values\nxgb_olt_scld = xgb.XGBRegressor(random_state = 111, max_depth = 2)\nxgb_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_xgb = xgb_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_xgb = xgb_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for XGBoost model on outlier treated data\")\nscld_olt_xgb_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_xgb)\nscld_olt_train_xgb_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_xgb)\nscld_olt_val_xgb_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_xgb)\nprint(\"Mean Squared Error :\", scld_olt_xgb_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_xgb_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_xgb_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Linear kernel\nsvr_linear_olt_scld = SVR(kernel = 'linear')\nsvr_linear_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_svr_linear = svr_linear_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_svr_linear = svr_linear_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with linear kernel on scaled outlier treated data\")\nscld_olt_svr_linear_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_svr_linear)\nscld_olt_train_svr_linear_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_svr_linear)\nscld_olt_val_svr_linear_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_svr_linear)\nprint(\"Root Mean Squared Error :\", scld_olt_svr_linear_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_svr_linear_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_svr_linear_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Polynomial kernel\nsvr_poly_olt_scld = SVR(kernel = 'poly')\nsvr_poly_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_svr_poly = svr_poly_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_svr_poly = svr_poly_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with polynomial kernel on sclaed outlier treated data\")\nscld_olt_svr_poly_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_svr_poly)\nscld_olt_train_svr_poly_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_svr_poly)\nscld_olt_val_svr_poly_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_svr_poly)\nprint(\"Root Mean Squared Error :\", scld_olt_svr_poly_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_svr_poly_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_svr_poly_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - Sigmoid kernel\nsvr_sigmoid_olt_scld = SVR(kernel = 'sigmoid')\nsvr_sigmoid_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_svr_sigmoid = svr_sigmoid_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_svr_sigmoid = svr_sigmoid_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with sigmoid kernel on sclaed outlier treated data\")\nscld_olt_svr_sigmoid_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_svr_sigmoid)\nscld_olt_train_svr_sigmoid_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_svr_sigmoid)\nscld_olt_val_svr_sigmoid_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_svr_sigmoid)\nprint(\"Root Mean Squared Error :\", scld_olt_svr_sigmoid_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_svr_sigmoid_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_svr_sigmoid_r2s)\nprint(\"**************\" * 7)\n\n# Support Vector Regression model - RBF kernel\nsvr_rbf_olt_scld = SVR(kernel = 'rbf')\nsvr_rbf_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_svr_rbf = svr_rbf_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_svr_rbf = svr_rbf_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for Support Vector Model with rbf kernel on sclaed data\")\nscld_olt_svr_rbf_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_svr_rbf)\nscld_olt_train_svr_rbf_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_svr_rbf)\nscld_olt_val_svr_rbf_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_svr_rbf)\nprint(\"Root Mean Squared Error :\", scld_olt_svr_rbf_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_svr_rbf_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_svr_rbf_r2s)\nprint(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:05.734813Z","iopub.execute_input":"2023-12-17T18:42:05.73556Z","iopub.status.idle":"2023-12-17T18:42:27.547145Z","shell.execute_reply.started":"2023-12-17T18:42:05.735513Z","shell.execute_reply":"2023-12-17T18:42:27.545858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"find_k(scld_olt_x_train, scld_olt_y_train, scld_olt_x_val, scld_olt_y_val)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:27.549077Z","iopub.execute_input":"2023-12-17T18:42:27.55018Z","iopub.status.idle":"2023-12-17T18:42:33.832737Z","shell.execute_reply.started":"2023-12-17T18:42:27.55013Z","shell.execute_reply":"2023-12-17T18:42:33.831666Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Lowest error rate occurs first at k = 15\n- So proceeding with k = 15 to build the model","metadata":{}},{"cell_type":"code","source":"# Building KNN regressor with k = 15\nknn_olt_scld = KNeighborsRegressor(n_neighbors = 15)\nknn_olt_scld.fit(scld_olt_x_train, scld_olt_y_train)\nscld_olt_y_pred_train_knn = knn_olt_scld.predict(scld_olt_x_train)\nscld_olt_y_pred_val_knn = knn_olt_scld.predict(scld_olt_x_val)\n\nprint(\"Accuracy Scores for KNN Regressor model on scaled raw data\")\nscld_olt_knn_mse = mean_squared_error(scld_olt_y_val, scld_olt_y_pred_val_knn)\nscld_olt_train_knn_r2s = r2_score(scld_olt_y_train, scld_olt_y_pred_train_knn)\nscld_olt_val_knn_r2s = r2_score(scld_olt_y_val, scld_olt_y_pred_val_knn)\nprint(\"Mean Squared Error :\", scld_olt_knn_mse)\nprint(\"R-squared Score (Train) :\", scld_olt_train_knn_r2s)\nprint(\"R-squared Score (Test) :\", scld_olt_val_knn_r2s)\nprint(\"**************\" * 7)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:33.834106Z","iopub.execute_input":"2023-12-17T18:42:33.83516Z","iopub.status.idle":"2023-12-17T18:42:34.223955Z","shell.execute_reply.started":"2023-12-17T18:42:33.835119Z","shell.execute_reply":"2023-12-17T18:42:34.22275Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Checking the accuracy of models on scled outlier treated data\n\nscld_olt_mse_list = [scld_olt_lr_mse, scld_olt_lar_mse, scld_olt_ridge_mse, scld_olt_enet_mse, scld_olt_dtree_mse, \n                     scld_olt_rf_mse, scld_olt_gb_mse, scld_olt_lgb_mse, scld_olt_xgb_mse, scld_olt_knn_mse, \n                     scld_olt_svr_linear_mse, scld_olt_svr_poly_mse, scld_olt_svr_sigmoid_mse, scld_olt_svr_rbf_mse]\n\nscld_olt_r2_train_list = [scld_olt_train_lr_r2s, scld_olt_train_lar_r2s, scld_olt_train_ridge_r2s, scld_olt_train_enet_r2s,\n                          scld_olt_train_dtree_r2s, scld_olt_train_rf_r2s, scld_olt_train_gb_r2s, scld_olt_train_lgb_r2s, \n                          scld_olt_train_xgb_r2s, scld_olt_train_knn_r2s, scld_olt_train_svr_linear_r2s, \n                          scld_olt_train_svr_poly_r2s, scld_olt_train_svr_sigmoid_r2s, scld_olt_train_svr_rbf_r2s]\n\nscld_olt_r2_test_list = [scld_olt_val_lr_r2s, scld_olt_val_lar_r2s, scld_olt_val_ridge_r2s, scld_olt_val_enet_r2s, \n                         scld_olt_val_dtree_r2s, scld_olt_val_rf_r2s, scld_olt_val_gb_r2s, scld_olt_val_lgb_r2s, \n                         scld_olt_val_xgb_r2s, scld_olt_val_knn_r2s, scld_olt_val_svr_linear_r2s, scld_olt_val_svr_poly_r2s, \n                         scld_olt_val_svr_sigmoid_r2s, scld_olt_val_svr_rbf_r2s]\n\ncheck_scores(scld_olt_mse_list, scld_olt_r2_train_list, scld_olt_r2_test_list, 'scaled outlier treated')","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:34.225753Z","iopub.execute_input":"2023-12-17T18:42:34.22614Z","iopub.status.idle":"2023-12-17T18:42:34.88311Z","shell.execute_reply.started":"2023-12-17T18:42:34.226105Z","shell.execute_reply":"2023-12-17T18:42:34.88175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- Gradient Boost Regressor and Random Forest perform well on both train and test sets, indicating good generalization.\n- Support Vector Models with linear and RBF kernels show good performance.\n- Polynomial and sigmoid kernels in Support Vector Models exhibit poor performance, possibly indicating overfitting or inappropriate kernel choices.\n- KNN Regressor shows good performance on the scaled and outlier-treated data.","metadata":{}},{"cell_type":"markdown","source":"#### Cross validation of the top 3 models","metadata":{}},{"cell_type":"code","source":"# Cross validation on Random Forest with scaled raw data\n\ntrain_rf_scld_raw = cross_val_score(rf_raw_scld, scld_raw_x_train, scld_raw_y_train, cv = 10, scoring = 'r2')\nval_rf_scld_raw = cross_val_score(rf_raw_scld, scld_raw_x_val, scld_raw_y_val, cv = 10, scoring = 'r2')\n\nprint(\"Cross validation on Random Forest with scaled raw data\")\nprint(\"Train Mean Accuracy\", train_rf_scld_raw.mean())\nprint(\"**************\")\nprint(\"Train Max Accuracy\", train_rf_scld_raw.max())\nprint(\"**************\"*7)\nprint(\"Test Mean Accuracy\", val_rf_scld_raw.mean())\nprint(\"**************\")\nprint(\"Test Max Accuracy\", val_rf_scld_raw.max())\nprint(\"**************\"*7)\n\nelapsed_time_rf_scld_raw = timeit.timeit()\nprint(f\"Elapsed Time: {elapsed_time_rf_scld_raw} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:34.885033Z","iopub.execute_input":"2023-12-17T18:42:34.885416Z","iopub.status.idle":"2023-12-17T18:42:58.142724Z","shell.execute_reply.started":"2023-12-17T18:42:34.885382Z","shell.execute_reply":"2023-12-17T18:42:58.141354Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation on Random Forest with scaled outlier treated data\n\ntrain_rf_scld_olt = cross_val_score(rf_olt_scld, scld_olt_x_train, scld_olt_y_train, cv = 10, scoring = 'r2')\nval_rf_scld_olt = cross_val_score(rf_olt_scld, scld_olt_x_val, scld_olt_y_val, cv = 10, scoring = 'r2')\n\nprint(\"Cross validation on Random Forest with scaled outlier treated data\")\nprint(\"Train Mean Accuracy\", train_rf_scld_olt.mean())\nprint(\"**************\")\nprint(\"Train Max Accuracy\", train_rf_scld_olt.max())\nprint(\"**************\"*7)\nprint(\"Test Mean Accuracy\", val_rf_scld_olt.mean())\nprint(\"**************\")\nprint(\"Test Max Accuracy\", val_rf_scld_olt.max())\nprint(\"**************\"*7)\n\nelapsed_time_rf_scld_olt = timeit.timeit()\nprint(f\"Elapsed Time: {elapsed_time_rf_scld_olt} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:42:58.144827Z","iopub.execute_input":"2023-12-17T18:42:58.145307Z","iopub.status.idle":"2023-12-17T18:43:21.381991Z","shell.execute_reply.started":"2023-12-17T18:42:58.145262Z","shell.execute_reply":"2023-12-17T18:43:21.380876Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Cross validation on Gradient Boosting with scaled outlier treated data\n\ntrain_gb_scld_olt = cross_val_score(gb_olt_scld, scld_olt_x_train, scld_olt_y_train, cv = 10, scoring = 'r2')\nval_gb_scld_olt = cross_val_score(gb_olt_scld, scld_olt_x_val, scld_olt_y_val, cv = 10, scoring = 'r2')\n\nprint(\"Cross validation on Gradient Boosting with scaled outlier treated data\")\nprint(\"Train Mean Accuracy\", train_gb_scld_olt.mean())\nprint(\"**************\")\nprint(\"Train Max Accuracy\", train_gb_scld_olt.max())\nprint(\"**************\"*7)\nprint(\"Test Mean Accuracy\", val_gb_scld_olt.mean())\nprint(\"**************\")\nprint(\"Test Max Accuracy\", val_gb_scld_olt.max())\nprint(\"**************\"*7)\n\nelapsed_time_gb_scld_olt = timeit.timeit()\nprint(f\"Elapsed Time: {elapsed_time_gb_scld_olt} seconds\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:21.383262Z","iopub.execute_input":"2023-12-17T18:43:21.383583Z","iopub.status.idle":"2023-12-17T18:43:40.045865Z","shell.execute_reply.started":"2023-12-17T18:43:21.383555Z","shell.execute_reply":"2023-12-17T18:43:40.044678Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"- The Random Forest models with both scaled raw data and scaled outlier-treated data have similar performance, with high accuracy on both training and test sets.\n- Gradient Boosting also performs well, with slightly higher mean accuracy on the training set compared to Random Forest.\n- The elapsed time for cross-validation is quite short, indicating efficient execution.\n- But considering all the factors Random Forest model with scaled raw data seems to be a strong performer:\n    - Test Mean Accuracy: 84.59%\n    - Test Max Accuracy: 86.54%\n    - Elapsed Time: 0.0222 seconds","metadata":{}},{"cell_type":"markdown","source":"#### Saving the model for deployment and genearting the output file","metadata":{}},{"cell_type":"code","source":"# Saving model for deployment\n\nfinal_model = rf_raw_scld\nfilename = 'Movie_Rating_Prediction.sav'\npickle.dump(final_model, open(filename, 'wb'))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.047432Z","iopub.execute_input":"2023-12-17T18:43:40.04873Z","iopub.status.idle":"2023-12-17T18:43:40.062813Z","shell.execute_reply.started":"2023-12-17T18:43:40.048682Z","shell.execute_reply":"2023-12-17T18:43:40.061511Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#rest_df, test_df = [x for y, x in movies_df.groupby(movies_df['Rating'].isna())]","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.064652Z","iopub.execute_input":"2023-12-17T18:43:40.065951Z","iopub.status.idle":"2023-12-17T18:43:40.071047Z","shell.execute_reply.started":"2023-12-17T18:43:40.06591Z","shell.execute_reply":"2023-12-17T18:43:40.069752Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Building the output file\n\nout_df = rest_df","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.072605Z","iopub.execute_input":"2023-12-17T18:43:40.07312Z","iopub.status.idle":"2023-12-17T18:43:40.084364Z","shell.execute_reply.started":"2023-12-17T18:43:40.073085Z","shell.execute_reply":"2023-12-17T18:43:40.083152Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values in the dataset\n\n# Filling missing years with the mode\n\nout_df['Year'] = out_df['Year'].str.extract('([0-9]+)').astype(int)\nmode_year_final = out_df['Year'].mode()\nout_df['Year'] = out_df['Year'].fillna(mode_year_final)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.086095Z","iopub.execute_input":"2023-12-17T18:43:40.086455Z","iopub.status.idle":"2023-12-17T18:43:40.120895Z","shell.execute_reply.started":"2023-12-17T18:43:40.086423Z","shell.execute_reply":"2023-12-17T18:43:40.119625Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values in duration cokumn with median\n\nout_df['Duration'] = out_df['Duration'].str.extract('([0-9]+)').astype(float)\nmedian_duration_final = out_df['Duration'].median()\nout_df['Duration'] = out_df['Duration'].fillna(median_duration_final)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.122762Z","iopub.execute_input":"2023-12-17T18:43:40.123147Z","iopub.status.idle":"2023-12-17T18:43:40.148589Z","shell.execute_reply.started":"2023-12-17T18:43:40.123114Z","shell.execute_reply":"2023-12-17T18:43:40.14725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling missing values in the genre column based on Mode Imputaion method \n\n# Already defined functions\n# def expand_genre(df):\n#     genres_df = df['Genre'].str.split(', ', expand = True)\n#     df = pd.concat([df, genres_df], axis = 1)\n#     df.rename(columns = {0 : 'Genre_1', 1 : 'Genre_2', 2 : 'Genre_3'}, inplace = True)\n#     df.drop('Genre', axis = 1, inplace = True)\n#     return df\n# def drop_genre(df):\n#     df.drop(['Genre_2','Genre_3'], axis = 1, inplace = True)\n#     df.rename(columns = {'Genre_1' : 'Genre'}, inplace = True)\n#     return df\n\nout_df = expand_genre(out_df)\nout_df = drop_genre(out_df)\n\nmode_per_year_final = out_df.groupby('Year')['Genre'].apply(lambda x: x.mode().iloc[0])    # Gives a df with node of each year\nout_df['Genre'] = out_df.apply(lambda row: mode_per_year[row['Year']] if pd.isnull(row['Genre']) else row['Genre'], axis=1)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.150603Z","iopub.execute_input":"2023-12-17T18:43:40.151061Z","iopub.status.idle":"2023-12-17T18:43:40.351512Z","shell.execute_reply.started":"2023-12-17T18:43:40.151023Z","shell.execute_reply":"2023-12-17T18:43:40.350435Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Filling the missing values in the names of Directors and actors as 'Not Available'\n\n# Already defined functions\n# def fill_names(df):\n#     df['Director'] = df['Director'].fillna('Not Available')\n#     df['Actor 1'] = df['Actor 1'].fillna('Not Available')\n#     df['Actor 2'] = df['Actor 2'].fillna('Not Available')\n#     df['Actor 3'] = df['Actor 3'].fillna('Not Available')\n#     return df\n\nrest_df = fill_names(rest_df)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.353149Z","iopub.execute_input":"2023-12-17T18:43:40.353486Z","iopub.status.idle":"2023-12-17T18:43:40.367355Z","shell.execute_reply.started":"2023-12-17T18:43:40.353457Z","shell.execute_reply":"2023-12-17T18:43:40.366046Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Droping the insignificant variables\n\nout_df = out_df.drop(['Name'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.369352Z","iopub.execute_input":"2023-12-17T18:43:40.370381Z","iopub.status.idle":"2023-12-17T18:43:40.381347Z","shell.execute_reply.started":"2023-12-17T18:43:40.370314Z","shell.execute_reply":"2023-12-17T18:43:40.380203Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Converting the Votes from object to int datatype\n\nout_df['Votes'] = out_df['Votes'].str.extract('([0-9]+)').astype(int)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.382874Z","iopub.execute_input":"2023-12-17T18:43:40.383744Z","iopub.status.idle":"2023-12-17T18:43:40.413587Z","shell.execute_reply.started":"2023-12-17T18:43:40.383708Z","shell.execute_reply":"2023-12-17T18:43:40.412379Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Using Target encoding to encode the categorical variables\n# As there are a diverese range of categorical variables\n\ncategorical_variables = out_df.select_dtypes(include = ['object']).columns.tolist()\nencoder_final = ce.TargetEncoder(cols = categorical_variables)\nencoded_df_final = encoder.fit_transform(out_df[categorical_variables], out_df['Rating'])\nencoded_df_final.columns = [f\"{col}_TargetEncoded\" for col in categorical_variables]\nout_df = pd.concat([out_df, encoded_df_final], axis = 1)\n\n# Droping the categorical variables and keeping the encoded columns\n\nout_df = out_df.drop(['Director', 'Actor 1', 'Actor 2', 'Actor 3', 'Genre'], axis = 1)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.415201Z","iopub.execute_input":"2023-12-17T18:43:40.415583Z","iopub.status.idle":"2023-12-17T18:43:40.585092Z","shell.execute_reply.started":"2023-12-17T18:43:40.415549Z","shell.execute_reply":"2023-12-17T18:43:40.58374Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Splitting dependent and independent variable\n\nx_final = out_df.drop(['Rating'], axis = 1)\ny_final = out_df['Rating']","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.586784Z","iopub.execute_input":"2023-12-17T18:43:40.587199Z","iopub.status.idle":"2023-12-17T18:43:40.594695Z","shell.execute_reply.started":"2023-12-17T18:43:40.587163Z","shell.execute_reply":"2023-12-17T18:43:40.593699Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Scaling the training dataset using standard scaler\n\nssc_final = StandardScaler()\nx_final = pd.DataFrame(ssc_final.fit_transform(x_final))","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.596713Z","iopub.execute_input":"2023-12-17T18:43:40.597108Z","iopub.status.idle":"2023-12-17T18:43:40.616619Z","shell.execute_reply.started":"2023-12-17T18:43:40.597072Z","shell.execute_reply":"2023-12-17T18:43:40.615572Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Predicting the Rating using Random Forest model that was chosen as final model\n\ny_pred = final_model.predict(x_final)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.618263Z","iopub.execute_input":"2023-12-17T18:43:40.618691Z","iopub.status.idle":"2023-12-17T18:43:40.703737Z","shell.execute_reply.started":"2023-12-17T18:43:40.618657Z","shell.execute_reply":"2023-12-17T18:43:40.702469Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_result = pd.DataFrame(y_pred)\nfinal_result = final_result.rename(columns = {0 : \"Predicted_Rating\"})\nfinal_result","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.705748Z","iopub.execute_input":"2023-12-17T18:43:40.706143Z","iopub.status.idle":"2023-12-17T18:43:40.724704Z","shell.execute_reply.started":"2023-12-17T18:43:40.70611Z","shell.execute_reply":"2023-12-17T18:43:40.723507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Preparing the final data fram for output\n\nfinal_result1 = pd.concat([(rest_df.drop(['Rating'], axis = 1)), rest_df['Rating'], pd.DataFrame(final_result)], axis = 1)\nfinal_result1.head(5)","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.726381Z","iopub.execute_input":"2023-12-17T18:43:40.727529Z","iopub.status.idle":"2023-12-17T18:43:40.759446Z","shell.execute_reply.started":"2023-12-17T18:43:40.72748Z","shell.execute_reply":"2023-12-17T18:43:40.757998Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_result1.to_csv(\"Indian_Movie_Rating_Prediction.csv\")","metadata":{"execution":{"iopub.status.busy":"2023-12-17T18:43:40.761747Z","iopub.execute_input":"2023-12-17T18:43:40.762646Z","iopub.status.idle":"2023-12-17T18:43:40.895Z","shell.execute_reply.started":"2023-12-17T18:43:40.762598Z","shell.execute_reply":"2023-12-17T18:43:40.894008Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}